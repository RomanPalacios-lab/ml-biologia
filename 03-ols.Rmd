# Capítulo 3: Regresiones lineales

Las regresiones lineales estan entre las herramientas más utilizadas para cuantificar y describir la asociacion entre variables o predecir los valores de una respuesta. En esta aproximacion metodologica, la respuesta en el modelo supervisado es continua. Los modelos de regresion lineal son usualmente percibidos como poco flexibles. Sin embargo, estos modelos pueden llegar a incluir niveles de flexibilidad y complejidad comparables con otros modelos usualmente vistos como mas flexibles. Los modelos lineales son un excelente punto de partida para entender métodos más avanzados, ya que, muchos de esos métodos son extensiones o casos especiales de regresiones lineales.

## Regresión lineal univariada

Este tipo de regresión es potencialmente la más simple que existe en su estructura. La regresion lineal univariada asume que hay una relación lineal entre el predictor $X$ y la variable continua de respuesta $Y$. Específicamente, este modelo asume que unicamente existe una dependencia entre $X$ y $Y$. Dentro de este paradigma, esta relación se representa como:

$$Y ≈ β_{0} + β_{1}X$$

En esta ecuacion, $β_{0}$ y $β_{1}$ representan dos parametros: intercepto y la pendiente, respectivamente. El intercepto se refiere al valor en el cual $x$ es igual a $0$, $x = 0$. La pendiente se refiere a al cambio de $y$ por una unidad en el cambio de $x$. Es entonces el ángulo de la línea en el plano. Estos parametros deben ser estimados a partir de los datos,

$$(x_1, y_1), (x_2, y_2), … , (x_n, y_n)$$

donde $n$ representa los pares de observaciones en el conjunto de datos, que consisten en una medida de $X$ y una de $Y$. Teniendo en cuenta que nuestro objetivo es el obtener los coeficientes para $β_{0}$ y $β_{1}$, asumiendo un ajuste a los datos de acuerdo a, 

$$\hat{y} ≈ \hat{β}_{0} + \hat{β}_{1}X_{i}$$

La ecuación anterior busca predecir $Y$ en base al valor iésimo de $X$. Con esto en mente, podemos decir entones que $e_{i} = y_{i} - \hat{y}_{i}$ es entonces la diferencia entre el valor real de cada observacion en el conjunto de datos ($y_{i}$) y el aproximado por el modelo ($\hat{y}_{i}$). Esta diferencia es conocida como residual e identifica el ajuste del modelo con los datos. Conociendo entonces la desviacion que existe entre los valores generados por el modelo y los datos colectados, podemos entonces enfocarnos en minimizar el error (e.g. residuales) a partir de cambios en los valores de los parametros $\hat{β}_{0}$ y $\hat{β}_{1}$.

### Mediciones de error para regresiones lineales univariadas para los coeficientes y el modelo

Lo que buscamos en una regresión lineal es ajustar un modelo donde se cometa el minimo error con respecto a los datos existentes. Hay muchas maneras de buscar la cercanía de estos puntos (i.e. los del modelo y las observaciones). Uno de los métodos más usados es conocido como mínimos cuadrados. Con esto en mente, podemos definir la suma de los cuadrados del error residual (**SCE** o **RSS**, residual sum of squares), como:

$$RSS = e_{1}^{2} + e_{2}^{2} +⋯+ e_{n}^{2}$$

La cual equivale a la suma a lo largo de todas las observaciones en el set de datos de los residuales al cuadrado:

$$RSS=[(y_{1}-\hat{β}_{0}-\hat{β}_{1}x_{1})]^{2}+[(y_{2}-\hat{β}_{0}-\hat{β}_{1}x_{2})]^{2}+⋯+[(y_{n}-\hat{β}_{0}-\hat{β}_{1}x_{n})]^{2}$$

Usualmente utilizamos **RSS** para determinar la proporción de la variación total que es explicada por el modelo de regresión ($R^{2}$ o coeficiente de determinación – vamos a hablar de esto un poco más adelante).

Tambien notamos que cuando trabajamos con funciones se asume que la relación mas cerca a la realidad tiene un termino de error ($ε$). Dado esto nuestra ecuación lineal estaría dada en realidad por la estructura,

$$Y ≈ β_{0} + β_{1}X + ε$$

Hasta el momento hemos descrito patrones lineales en el set de datos. Sin embargo, los modelos que subyacen datos reales raramente son lineales. Esto es debido a complejidad adicional, donde variables externas pueden generar efectos sobre la estructura de relacion entre las variables focales. Por ello, la ecuación que mostramos anteriormente es la mejor aproximación lineal para encontrar la relación entre $X$ y $Y$. Esto implica que, dado los coeficientes del modelo son desconocidos en datos experimentales, solo podemos estimar estos parametros a partir de aproximaciones como las de minimos cuadrados,

$$\hatβ_{1} = \frac{\sum_{i = 1}^{n}(x_{i} - \overline{x})(y_{i} - \overline{y})}{\sum_{i = 1}^{n}(x_{i} - \overline{x})^{2}}$$,

$$\hatβ_{0} = \overline{y} - \hatβ_{1}\overline{x}$$

Donde $\overline{y} ≡ \frac{1}{n}\sum_{i = 1}^{n}y_{i}$ y $\overline{x} ≡ \frac{1}{n}\sum_{i = 1}^{n}x_{i}$ ($≡$ significa *equivalente a*) son medias. Por lo tanto, lo que se busca es estimar dos coeficientes desconocidos usando $β_{0}$ y $β_{1}$, los cuales definen directamente la línea a partir de la aproximacion de minimos cuadrados. *Nota: si vemos con detenimiente esta ecuación, es una simplificación de la ecuación de RSS*.

Otra variable a tener en cuenta cuando estemos haciendo nuestros análisis es $\mu$ o la media. Este parametro, el cual es desconocido, puede ser aproximado a partir de la media para nuestra muestra, $\hat{\mu} = \overline{y}$ donde $\overline{y} = \frac{1}{n}\sum_{i = 1}^{n}y_{i}$. Para saber que tan precisa es la media de la muestra con respecto a la media global podemos estimar el error estándar de $\hat{\mu}$, que se puede representar como $SE(\hat{\mu})$, cuya formula es

$$Var(\hat{\mu}) = SE(\hat{\mu})^{2} = \frac{\delta^{2}}{n}$$,

Donde $\delta$ es la desviación estándar para cada uno de los valores de $y_{i}$ de $Y$. Igualmente, $\delta^{2}$ hace referencia a la varianza del error. Para esto tenemos que asumir que el error $ε_{i}$ de cada observación están no correlacionados, es decir, $\delta^{2} = Var(ε)$. Generalmente, $\delta^{2}$ es desconocida, pero podemos estimar el error estánadar de los residuos o RSE (por sus siglas en inglés *residual standard error*), para esto usamos la siguiente formula, 

$$RSE = \sqrt{RSS/(n - 2)}$

Ya teniendo el error estándar, podemos estimar el intervalo de confianza (e.g. 95%). Dentro de este rango se encuentra el valor que no conocemos para nuestro parámetro de interes con el valor definido de confianza. Para un regresión lineal, el 95% del intervalo de confianza de $\hatβ_{1}$ está dado por,

$$\hatβ_{1} ± 2 ⋅ SE(\hatβ_{1})$$

$$[\hatβ_{1} - 2 ⋅ SE(\hatβ_{1})]  ,  [\hatβ_{1} + 2 ⋅ SE(\hatβ_{1})]$$

Igualmente, el intervalo de confianza de $\hatβ_{0}$, esta dado por una ecuación similar:

$$\hatβ_{0} ± 2 ⋅ SE(\hatβ_{0})$$

Tras estimar el error estandard en los parametros, podemos consider probar hipótesis en los coeficientes. La hipótesis que usualmente probamos contrastamos es la **hipótesis nula** que establece,

$H_{0}$: No hay relación entre $X$ y $Y$

Mientras que la **hipótesis alternativa** indica que,

$H_{\alpha}$: Hay relación entre $X$ y $Y$

Matemáticamente hablando, la denotación para estas dos hipótesis sería respectivamente:

$$H_{0}: \hatβ_{0} = 0$$

Y

$$H_{\alpha}: \hatβ_{1} ≠ 0$$

La pregunta que queda ahora es ¿cómo probamos (rechazamos o no) la *hipótesis nula*?. Para este tenemos que determinar que tan alejado $\hatβ_{1}$, la pendiente, está de un valor de 0. Para esto utilizamos el **estadístico t**:

$$t = \frac{\hatβ_{1} - 0}{SE(\hatβ_{1})}$$, 

Este estadístico mide el número de desvaciones estándar en el que $\hatβ_{1}$ se aleja de 0. Por lo tanto, valores pequeños de $\hatβ_{1}$ dan evidencia de que $\hatβ_{1} ≠ 0$, es decir hay una relación estadistica entre $X$ y $Y$. Cuando $\hatβ_{1}$ tiene valores absolutos muchos más grande, se rechaza la *hipótesis nula*. 

Es relativamente sencillo computar la probabilidad de observar un valor de $|t|$ o mayor, cuando asumimos que $\hatβ_{1} = 0$. A esta probabilidad se le conoce normalmente como *valor-p*. Cuando el valor de *p* es muy pequeño (e.g. p<0.05) *rechazamos la hipótesis nula*, es decir, hay evidencia de relación entre $X$ y $Y$. Si el *valor-p* es mas grande (e.g. >0.05), no se rechaza la hipotesis nula. 

Otro estadístico a tener en cuenta es el estadístico $R^{2}$, comunmente conocido como **coeficiente de proporcionalidad**. Esta se usa como una manera alternativa de ver que tan bien ajustado está el modelo. $R^{2}$ usualmente toma valores entre 0 y 1, y es independiente de la escala de $Y$. El coefiente de proporcionalidad frecuentemente se calcula como:

$$R^{2} = \frac{TSS-RSS}{RSS} = 1 - \frac{RSS}{TSS}$$

Donde $TSS = \sum(y_{1} - \overline{y})^{2}$ esla suma total de todos las cuadrados. Este indice mide la varianza total de la respuesta $Y$ o la cantidad de variabilidad de la respuesta antes de la regresión. Por otra parte, el RSS mide la cantidad de variabilidad que no se explica luego de realizar la regresión. Asumiendo estas definiciones podemos decir que $TSS - RSS$ mide la la cantidad de variabilidad de la respues que es explicada a causa de la regresión. Por lo tanto, $R^{2}$ *mide la proporción de la variablidad de $Y$ que es explicada por $X$*. Cuando $R^{2}$  es cercano 1, una gran proporción de la variablidad en la respuesta es explicada por el modelo. Un valor de $R^{2}$ cercano a 0 indicaría que la regresió no explica la variabilidad de la respuesta.


## Regresión lineal de multiples variables

Comunmente, las asociaciones entre variables no son unicamente entre dos variables. Por lo tanto, las respuestas a predicciones están dadas por más de un predictor. Para esto usualmente utilizamos una regresión de multiples variables. En este paradigma, la relacion entre cada predictor y la respuesta esta modulada por su propio parametro (i.e. pendiente). Este tipo de regresión lineal está dada por una cantidad de predictores *p*, y se denota matemáticamente de la siguiente manera,

$$Y ≈ β_{0} + β_{1}X_{1} + β_{2}X_{2} + … + β_{p}X_{p} + ε$$

Donde $X_{j}$ representa del $j-ésimo$ predictor y $β_{j}$ cuantifica la asociación entre la variable y la respuesta. Esto se puede interpretar como $β_{j}$ es el efecto *promedio* de $Y$ por cada unidad de incremento en $X_{j}$ cuando *cuando todos los otros predictores se mantienen constantes*. Excepto por la asuncion de constancia, esta definicion de pendientes no difiere marcadamente en interpretacion con relacion a lo discutido anteriormente sobre regresiones lineales. Al igual que en la regresión lineal de una varible, los coeficientes de regresión $β_{0}$, $β_{1}$,..., $β_{p}$ son desconocidos, por lo tanto, lo que estimamos es $\hatβ_{0}$, $\hatβ_{1}$,..., $\hatβ_{p}$, lo cual hace que nuestra formula de regresión lineal cambie un poco

$$\hat{y} = \hatβ_{0} + \hatβ_{1}x_{1} + \hatβ_{2}x_{2} + … + \hatβ_{p}X_{p} + ε$$

Por lo tanto, en regresiones multiples, existen tantas pendientes como predictores en la function. Es entonces extender, de alguna forma, el resultado de extender un modelo de regresion simple para tener en cuenta un mayor numero de variables predictoras. Hay que tambien anotar que este tipo de regresion es diferente a los modelos de regresion multivariada. En estos ultimos, existen multiples variables de respuesta, en vez de solo una como es el caso de la regresion multiple.


### Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo

Similar a lo que se discutia sobre regresiones lineales simples, los parámetros en estos modelos son estimados utilizando mínimos cuadros. Sin embargo, en este caso, utilizamos la siguente formula,

$$RSS ≡ \sum_{i = 1}^{n}(y_{i} - \hat{y}_{i})^{2}$$

$$RSS ≡ \sum_{i = 1}^{n}(y_{i} - \hatβ_{0} - \hatβ_{1}x_{i1} - \hatβ_{2}x_{i2} - … - \hatβ_{p}X_{ip})^{2}$$

Los valores de $β_{0}$, $β_{1}$,..., $β_{p}$ que minimizan la ecuación anterior son coeficientes estimados de regresión de los mínimos cuadradros multiples. Existen alternativas para estimar estos coeficientes como maxima varosimilitud.


### Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo

Las regresiones multiples tambien tienen hipótesis asociadas a las relaciones entre variables. Existen dos niveles de hipotesis en este tipo de regresiones. Primero, existe una relacion general entre los predictores y la respuesta? Segundo, existe una relacion particular entre cada predictor y la respuesta? Por ahora, nos enfocaremos en el primer aspecto. El segundo aspecto es congruente con las interpretaciones discutidas en regresiones de una variable (excepto por la asuncion de constancia en la interpretacion).

Nos preguntarnos por lo tanto si todos los coeficientes de la regresión son igual a cero, es decir, si $β_{1} = β_{2} = ... = β_{p} = 0$. Dado esto, la *hipótesis nula* sería,

$H_{0}: β_{1} = β_{2} = ... = β_{p} = 0$

Y la *hipótesis alternativa*,

$H_{\alpha}:$ Donde por lo menos uno de los $β_{j} = 0$, 

Para probar estas hipótesis utilizamos el **estadístico F**. Este estadistico se define matemáticamente de la siguiente manera:

$$F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}$$

Donde, $TSS = \sum(y_{i} - \overline{y})^2$ y $RSS = \sum(y_{i} - \hat{y_{i}})^2$. Si las asunciones del modelo son correctas entonces podemos decir que,

$$E[RSS/(n-p-1)] = \delta^{2}$$

Además, si $H_{0}$ es verdadera, entonces,

$$E[(TSS-RSS)/p] = \delta^{2}$$

Esto quiere decir que cuando no hay una relación estadistica entre la repuesta y los predictores, el *estadístico F* tomaría valores cerca a 1. Sin embargo, si la hipotesis nula fuese rechazada, entocnes el *estadístico F* tomaría valores mayores a 1, es decir, $E[(TSS - RSS)/p] > \alpha^{2}$.

Igualmente, a veces queremos encontrar si los coeficientes de un subcojunto *q* son iguales a cero. Para el caso de la hipótesis nula esto se escribiría matemáticamente como,

$$H_{0}: β_{p-q+1} = β_{p-q+2} += … = β_{p} = 0$$

Si queremos probar una modelo que contenga todas las variables *excepción* de las *q*. Supongamos que la suma de cuadrados de los residuos para este modelo es $RSS_{0}$, entonces la formar de calcular el *estadístico F* sería.

$$F = \frac{(RSS_{0} - RSS)/q}{RSS/(n-p-1)}$$

Algo que debemos tener en cuenta cuando estemos realizando la elección de variables en una regresión lineal de multiples variables funciona para cada asociación entre los predicttores y la variable respuuesta, cuando *p* es relativamente pequeño e casi tan pequeño como *n*. Cuando $p > n$ hay más coificientes $β_{j}$ para estimar que el número de observaciones que se necesita para estimar esos coeficientes. Dado este caso no podemos usar una regresión lineal multiple usando mínimos caudrados, por ello debemos usar otras alternativas, entre las cuales se encuentran: seleccion hacia adelante, seleccion desde atras y seleccion mixta, de las cuales hablaremos más adelante.

### Interacciones entre variables

Usar una de las bases de datos para ejemplificar esto. Hablar sobre como los resultados pueden cambiar cuando se usar una o varias variables; discutir esto cuando se tienen las interacciones y cuando no se tienen encuenta.

### Comparaciones y decisiones entre modelos de multiples variables

Hablar del valor p (NOTA: Ya hablé de este un poquito en regresiones de una variables). Introducir a metodos para decidir cual es el mejor modelo (AICc, AIC, BIC, adjusted R2, Mallow's C), aunque creo que esto lo vamos expandir mas un poco mas adelante.Hablar sobre seleccion de varibles: seleccion hacia adelante, seleccion desde atras, seleccion mixta.

### Asunsiones y posibles problemas de las regresiones lineales y cómo lidiar con ellas

Hablar sobre no linearidad, correlaciones de los errores, variancia no constante en el error, datos atipicos, Punto de alto apalancamiento (high-leverage) y colinearidad. Podriamos mostrar ejemplos a acá tambien.


- Definiciones generales (teorico):
-- Simple
-- Multiple
--- Distinguir de multivariada
-- Asunciones
-- Confounders (factores de confusion: En el libro no lo explican si no hasta que se abran de Clasificaciones)
-- Interacciones
-- data augmentation (Este no lo hablan el cap de regresiones)
-- Mediciones de error (RMSE; MSE; MAE; etc)
-- Comparaciones multimodelo (AIC, AICc, BIC)

- Implementacion (codigo y explicacion):
-- Ejemplo de reg simple
-- Ejemplo de reg multiple
-- Data augmentation
--- Mediciones de error y comparacion multimodelo
