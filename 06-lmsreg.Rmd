# Selección de modelos lineares y regularización

En este capítulos vamos a extender lo que sabemos hasta ahora de modelos lineales. En otras palabras, vamos ahabalr sobre cómmo podemos mejorar los modelos lineales simples, por medio de reemplazar los ajuste de mínimos caudrados con procedimientos alternativos de ajuste. Lo cual hace que haya una mejor la *precisión de la predicción* y la *interpretabilidad del modelo*.

* *Precisión de la predicción*: Esto provee la relación verdadera entre la respuesta y los predictores es aproximadamente lineal, donde los estimados de mínimos cuadrados tendrían un bajo sesgo. Cuando el número de observaciones *n* es mayor al número de variables *p*, entonces los estimados de mínimos cuadrados van a tender a tener baja varianza, lo cual causa que este sea efectivo en observaciones de prueba. Mientras que cuando *n* no es mucho más grande que *p*, entonces hay una alta variablilldad en el ajuste de los mínimos cuadrados, lo cual resulta en una sobreestimación y baja predictabilidad en observaciones futuras que no fueron utilizadas en el modelo. Cuando el número de predictores es mayor que el número de observaciones, no hay un solo estimado del coeficiente de mínimos cuadrados, es decir, la varianza es infinita, lo cual no permite que se puede usar este método. Por medio de *restringir* o *contraer* los coeficientes estimados podemos reducir la varianza a costo de aumentar el sesgo. Esto puede dar como resultados el mejorar al precisión, lo cual permite predecir la respoestado de observaciones que no fueron usadas en el modelo de entrenamiento.

* *Interpretabilidad del modelo*: 

## Selección de subconjuntos



### Selección del mejor subconjunto

### Selección paso a paso

#### Selección hacia adelante

#### Selección hacia atras

#### Selección híbrida



### Selección del modelo óptimo

#### $C_{p}$, $AIC$, $BIC$ y $R^{2}$ ajustado

#### Validación y validación cruzada



## Métodos de contracción

### Regresión de crestas (ridge regression)

#### ¿Por qué la regresión de crestas es *mejor* que los mínimos cuadrados?

### El Lasso

#### Otra formulación para regresión de crestas y El Lasso

#### Propiedad de selección de variables de The Lasso

#### Comparación entre El Lasso y la regresión de crestas

####  Un caso especies para la regresión de crestas y El Lasso

#### Interpretación Bayesiana para la regresión de crestas y El Lasso

### Selección de ajuste de parámetros



## Métodos de reducción dimensiones

### Regresión de componentes principales

#### Una descripción general del análisis de componentes principales

#### El enfoque de regresión de componentes principales

### Mínimos cuadrados parciales


## Consideraciones en dimensiones altas

### Datos de alta dimensionalidad

### ¿Qué hay de malo con la alta dimensionalidad?

### Regresión en alta dimensionalidad

### Interpretando los resultados en alta dimensionalidad