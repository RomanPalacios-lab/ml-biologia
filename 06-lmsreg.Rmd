# Selección de modelos lineares y regularización

En este capítulos vamos a extender lo que sabemos hasta ahora de modelos lineales. En otras palabras, vamos ahabalr sobre cómmo podemos mejorar los modelos lineales simples, por medio de reemplazar los ajuste de mínimos caudrados con procedimientos alternativos de ajuste. Lo cual hace que haya una mejor la *precisión de la predicción* y la *interpretabilidad del modelo*.

* *Precisión de la predicción*: Esto provee la relación verdadera entre la respuesta y los predictores es aproximadamente lineal, donde los estimados de mínimos cuadrados tendrían un bajo sesgo. Cuando el número de observaciones *n* es mayor al número de variables *p*, entonces los estimados de mínimos cuadrados van a tender a tener baja varianza, lo cual causa que este sea efectivo en observaciones de prueba. Mientras que cuando *n* no es mucho más grande que *p*, entonces hay una alta variablilldad en el ajuste de los mínimos cuadrados, lo cual resulta en una sobreestimación y baja predictabilidad en observaciones futuras que no fueron utilizadas en el modelo. Cuando el número de predictores es mayor que el número de observaciones, no hay un solo estimado del coeficiente de mínimos cuadrados, es decir, la varianza es infinita, lo cual no permite que se puede usar este método. Por medio de *restringir* o *contraer* los coeficientes estimados podemos reducir la varianza a costo de aumentar el sesgo. Esto puede dar como resultados el mejorar al precisión, lo cual permite predecir la respoestado de observaciones que no fueron usadas en el modelo de entrenamiento.

* *Interpretabilidad del modelo*: Esto se refiere a eliminar variables *innecesarias* que solo le agregan complejidad al modelo -por medio de ajustar los coeficientes de los estimados a cero. De esta manera, el modelo se vuelvo muchos mas interpretable. No obstante, hay que tener en cuenta que en los mínimos cuadrados es poco pobable obtener estimados de coeficientes que den exactamente cero.

Hay diferentes alternativas para ajustar el uso de los mínimos cuadrados.

* *Selección de subconjunto*: Este procedimiento trata de identificar un subconjunto de *p* predictores que se cree que están relacionados con la respuesta, para luego ajustar el modelo usado en los mínimos caudrados para reducir el conjunto de variables.

* *Contracción*: Este procedimiento requiere ajustar un modelo con todos *p* predictores. Lo que hace ue haya un efecto en la reducción de la varianza, a su vez, de pendiendo del tipo de contracción que se utilice algunos de los coeficientes puede ser estimados como cero, lo que hace que el método de contracción pueda usarse para la selección de variables.

* *Reducción dimensiones*: Este método tiene como objetivo *proyectar* los predictores *p* en *M* dimensiones subespaciales donde $M < p$. Lo cual se logra por medio de computar *M* *combinaciones lineares* diferentes o *proyecciones* de las variables. Luego, esta *M* proyecciones se usan como predictores para ajustar el modelos de regresión lineal por medio de mínimos cuadrados.


## Selección de subconjuntos

En esta sección vamos a hablar de dos tipos de selección de subconjuntos: selección del mejor subconjunto y selección paso a paso.

### Selección del mejor subconjunto

Para llevar a cabo este método, debemos ajustar separadamente regresiones de mínimos caudrados para cada combinación de predictores *p*. Es decir, ajustar modelos que tenga un predictor, luego para todos los modelos que contengan dos predictores $\binom{p}{2} = p(p-1)/2$, y así sucesivamente. Luego, revisamos todos los modelos resultantes para identificar el mejor modelo. No obstante, debemos tener en cuenta que este tipo de selección puede ser problemático porque pueden haber $2^p$ posibilidades que debemos considerar para poder elegir el mejor subconjunto.

Para llevar a cabo la selección del mejor subconjunto necesitamos seguir los siguientes pasos:


1) AAA
2) Para $k = 1, 2, ...p:$
    a) Ajustar todos los modelos $\binom{p}{k}$ que contengan el predictor *k*.
    b) Elegir el modelos entre todos los modelos $\binom{p}{k}$, a este lo llamaremos $M_{k}$
3)


### Selección paso a paso

#### Selección hacia adelante

#### Selección hacia atras

#### Selección híbrida



### Selección del modelo óptimo

#### $C_{p}$, $AIC$, $BIC$ y $R^{2}$ ajustado

#### Validación y validación cruzada



## Métodos de contracción

### Regresión de crestas (ridge regression)

#### ¿Por qué la regresión de crestas es *mejor* que los mínimos cuadrados?

### El Lasso

#### Otra formulación para regresión de crestas y El Lasso

#### Propiedad de selección de variables de The Lasso

#### Comparación entre El Lasso y la regresión de crestas

####  Un caso especies para la regresión de crestas y El Lasso

#### Interpretación Bayesiana para la regresión de crestas y El Lasso

### Selección de ajuste de parámetros



## Métodos de reducción dimensiones

### Regresión de componentes principales

#### Una descripción general del análisis de componentes principales

#### El enfoque de regresión de componentes principales

### Mínimos cuadrados parciales


## Consideraciones en dimensiones altas

### Datos de alta dimensionalidad

### ¿Qué hay de malo con la alta dimensionalidad?

### Regresión en alta dimensionalidad

### Interpretando los resultados en alta dimensionalidad