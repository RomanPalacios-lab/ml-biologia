# Selección de modelos lineares y regularización

En este capítulos vamos a extender lo que sabemos hasta ahora de modelos lineales. En otras palabras, vamos ahabalr sobre cómmo podemos mejorar los modelos lineales simples, por medio de reemplazar los ajuste de mínimos caudrados con procedimientos alternativos de ajuste. Lo cual hace que haya una mejor la *precisión de la predicción* y la *interpretabilidad del modelo*.

* *Precisión de la predicción*: Esto provee la relación verdadera entre la respuesta y los predictores es aproximadamente lineal, donde los estimados de mínimos cuadrados tendrían un bajo sesgo. Cuando el número de observaciones *n* es mayor al número de variables *p*, entonces los estimados de mínimos cuadrados van a tender a tener baja varianza, lo cual causa que este sea efectivo en observaciones de prueba. Mientras que cuando *n* no es mucho más grande que *p*, entonces hay una alta variablilldad en el ajuste de los mínimos cuadrados, lo cual resulta en una sobreestimación y baja predictabilidad en observaciones futuras que no fueron utilizadas en el modelo. Cuando el número de predictores es mayor que el número de observaciones, no hay un solo estimado del coeficiente de mínimos cuadrados, es decir, la varianza es infinita, lo cual no permite que se puede usar este método. Por medio de *restringir* o *contraer* los coeficientes estimados podemos reducir la varianza a costo de aumentar el sesgo. Esto puede dar como resultados el mejorar al precisión, lo cual permite predecir la respoestado de observaciones que no fueron usadas en el modelo de entrenamiento.

* *Interpretabilidad del modelo*: Esto se refiere a eliminar variables *innecesarias* que solo le agregan complejidad al modelo -por medio de ajustar los coeficientes de los estimados a cero. De esta manera, el modelo se vuelvo muchos mas interpretable. No obstante, hay que tener en cuenta que en los mínimos cuadrados es poco pobable obtener estimados de coeficientes que den exactamente cero.

Hay diferentes alternativas para ajustar el uso de los mínimos cuadrados.

* *Selección de subconjunto*: Este procedimiento trata de identificar un subconjunto de *p* predictores que se cree que están relacionados con la respuesta, para luego ajustar el modelo usado en los mínimos caudrados para reducir el conjunto de variables.

* *Contracción*: Este procedimiento requiere ajustar un modelo con todos *p* predictores. Lo que hace ue haya un efecto en la reducción de la varianza, a su vez, de pendiendo del tipo de contracción que se utilice algunos de los coeficientes puede ser estimados como cero, lo que hace que el método de contracción pueda usarse para la selección de variables.

* *Reducción dimensiones*: Este método tiene como objetivo *proyectar* los predictores *p* en *M* dimensiones subespaciales donde $M < p$. Lo cual se logra por medio de computar *M* *combinaciones lineares* diferentes o *proyecciones* de las variables. Luego, esta *M* proyecciones se usan como predictores para ajustar el modelos de regresión lineal por medio de mínimos cuadrados.


## Selección de subconjuntos

En esta sección vamos a hablar de dos tipos de selección de subconjuntos: selección del mejor subconjunto y selección paso a paso.

### Selección del mejor subconjunto

Para llevar a cabo este método, debemos ajustar separadamente regresiones de mínimos caudrados para cada combinación de predictores *p*. Es decir, ajustar modelos que tenga un predictor, luego para todos los modelos que contengan dos predictores $\binom{p}{2} = p(p-1)/2$, y así sucesivamente. Luego, revisamos todos los modelos resultantes para identificar el mejor modelo. No obstante, debemos tener en cuenta que este tipo de selección puede ser problemático porque pueden haber $2^p$ posibilidades que debemos considerar para poder elegir el mejor subconjunto.

Para llevar a cabo la selección del mejor subconjunto necesitamos seguir los siguientes pasos:


1) Supongamos que $M_{0}$ es el *modelo nulo*, el cual contiene no predictores. Este modelo solamente predice la media de muestra para cada observación.
2) Para $k = 1, 2,..., p:$
    a) Ajustar todos los modelos $\binom{p}{k}$ que contengan el predictor *k*.
    b) Elegir el modelos entre todos los modelos $\binom{p}{k}$, a este lo llamaremos $M_{k}$. En este paso, el *mejor* modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el $R^2$ más grande.
3) En este último paso, seleccionamos el mejor modelos entre $M_{0},...,M_{p}$ usando validación cruzada para predecir el error, $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado.

Aunque la selección del mejor subconjunto es muy utilizada, usualmente es computalmente difícil de realizar si el número de predictores es muy alto. Cuando *p* es muy grande, mayor el espacio de búsqueda -esto puede sobre estimar los datos y aumentar la varianza de los coeficientes de los estimados-, más alta será la probabilidad de encontrar un model que se vea bien en los datos de entrenamiento, aunque puede que no tenga el mayor poder predictivo sobre los datos futuros.


### Selección paso a paso

Debido a las diferentes complicaciones que tiene la *selección del mejor subconjunto*, se tienen otro tipo de métodos que usan modelos más restrictivo, lo cual los hace alternativas muchas más atractivos que la *selección del mejor subconjunto*. Este método es conocido como *selección paso a paso*, dentro de este hay tres diferentes tipos: selección hacia adelante, selección hacia atrás y selección hídrida.

#### Selección hacia adelante

Este tipo de selección tiene el nombre de selección hacia a delanta, porque comienza con un modelo que no contiene ningún predictor al comienzo, pero a medida que el modelo está corriendo se añaden variables al modelo una a la vez, hasta que todos las predictores estén dentro del modelo. Particularmente, solo las variables que causen un *mejoramiento adicional* al ajuste del modelo. este modelo funciona de la siguiente manera:

1) Supongamos que $M_{0}$ es el *modelo nulo*, el cual contiene no predictores.
2) Para $k = 0, 2,..., p - 1:$
    2.1) Ajustar todos los modelos $p - k$ que aumenten los predictores $M_{k}$ con un predictor adicional.
    2.2) Elegir el modelos entre todos los modelos $p - k$, a este lo llamaremos $M_{k + 1}$. En este paso, el *mejor* modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el $R^2$ más grande.
3) En este último paso, seleccionamos el mejor modelos entre $M_{0},...,M_{p}$ usando validación cruzada para predecir el error, $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado.

Selección hacia adelante tiene muchas ventajas con respecto a la selección del mejor subconjunto, sin embargo, a veces no garantiza encontrar el mejor modelo posible paara todos los moldes $2^{p}$ que contiene el subconjunto de predictores *p*. Para explicar esto, supongamos que tenemos un conjunto de datos con $p = 3$ predicotres, donde el mejor modelo posible de una variable contiene $X_{1}$

#### Selección hacia atrás

#### Selección híbrida



### Selección del modelo óptimo

#### $C_{p}$, $AIC$, $BIC$ y $R^{2}$ ajustado

#### Validación y validación cruzada



## Métodos de contracción

### Regresión de crestas (ridge regression)

#### ¿Por qué la regresión de crestas es *mejor* que los mínimos cuadrados?

### El Lasso

#### Otra formulación para regresión de crestas y El Lasso

#### Propiedad de selección de variables de The Lasso

#### Comparación entre El Lasso y la regresión de crestas

####  Un caso especies para la regresión de crestas y El Lasso

#### Interpretación Bayesiana para la regresión de crestas y El Lasso

### Selección de ajuste de parámetros



## Métodos de reducción dimensiones

### Regresión de componentes principales

#### Una descripción general del análisis de componentes principales

#### El enfoque de regresión de componentes principales

### Mínimos cuadrados parciales


## Consideraciones en dimensiones altas

### Datos de alta dimensionalidad

### ¿Qué hay de malo con la alta dimensionalidad?

### Regresión en alta dimensionalidad

### Interpretando los resultados en alta dimensionalidad