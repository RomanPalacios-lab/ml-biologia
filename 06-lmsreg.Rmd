# Selección de modelos lineares y regularización

En este capítulos vamos a extender lo que sabemos hasta ahora de modelos lineales. En otras palabras, vamos ahabalr sobre cómmo podemos mejorar los modelos lineales simples, por medio de reemplazar los ajuste de mínimos caudrados con procedimientos alternativos de ajuste. Lo cual hace que haya una mejor la *precisión de la predicción* y la *interpretabilidad del modelo*.

* *Precisión de la predicción*: Esto provee la relación verdadera entre la respuesta y los predictores es aproximadamente lineal, donde los estimados de mínimos cuadrados tendrían un bajo sesgo. Cuando el número de observaciones *n* es mayor al número de variables *p*, entonces los estimados de mínimos cuadrados van a tender a tener baja varianza, lo cual causa que este sea efectivo en observaciones de prueba. Mientras que cuando *n* no es mucho más grande que *p*, entonces hay una alta variablilldad en el ajuste de los mínimos cuadrados, lo cual resulta en una sobreestimación y baja predictabilidad en observaciones futuras que no fueron utilizadas en el modelo. Cuando el número de predictores es mayor que el número de observaciones, no hay un solo estimado del coeficiente de mínimos cuadrados, es decir, la varianza es infinita, lo cual no permite que se puede usar este método. Por medio de *restringir* o *contraer* los coeficientes estimados podemos reducir la varianza a costo de aumentar el sesgo. Esto puede dar como resultados el mejorar al precisión, lo cual permite predecir la respoestado de observaciones que no fueron usadas en el modelo de entrenamiento.

* *Interpretabilidad del modelo*: Esto se refiere a eliminar variables *innecesarias* que solo le agregan complejidad al modelo -por medio de ajustar los coeficientes de los estimados a cero. De esta manera, el modelo se vuelvo muchos mas interpretable. No obstante, hay que tener en cuenta que en los mínimos cuadrados es poco pobable obtener estimados de coeficientes que den exactamente cero.

Hay diferentes alternativas para ajustar el uso de los mínimos cuadrados.

* *Selección de subconjunto*: Este procedimiento trata de identificar un subconjunto de *p* predictores que se cree que están relacionados con la respuesta, para luego ajustar el modelo usado en los mínimos caudrados para reducir el conjunto de variables.

* *Contracción*: Este procedimiento requiere ajustar un modelo con todos *p* predictores. Lo que hace ue haya un efecto en la reducción de la varianza, a su vez, de pendiendo del tipo de contracción que se utilice algunos de los coeficientes puede ser estimados como cero, lo que hace que el método de contracción pueda usarse para la selección de variables.

* *Reducción dimensiones*: Este método tiene como objetivo *proyectar* los predictores *p* en *M* dimensiones subespaciales donde $M < p$. Lo cual se logra por medio de computar *M* *combinaciones lineares* diferentes o *proyecciones* de las variables. Luego, esta *M* proyecciones se usan como predictores para ajustar el modelos de regresión lineal por medio de mínimos cuadrados. No obstante, en este librillo no vamos a habalr mucho sobre este método de regularización.


## Selección de subconjuntos

En esta sección vamos a hablar de dos tipos de selección de subconjuntos: selección del mejor subconjunto y selección paso a paso.

### Selección del mejor subconjunto

Para llevar a cabo este método, debemos ajustar separadamente regresiones de mínimos caudrados para cada combinación de predictores *p*. Es decir, ajustar modelos que tenga un predictor, luego para todos los modelos que contengan dos predictores $\binom{p}{2} = p(p-1)/2$, y así sucesivamente. Luego, revisamos todos los modelos resultantes para identificar el mejor modelo. No obstante, debemos tener en cuenta que este tipo de selección puede ser problemático porque pueden haber $2^p$ posibilidades que debemos considerar para poder elegir el mejor subconjunto.

Para llevar a cabo la selección del mejor subconjunto necesitamos seguir los siguientes pasos:


1) Supongamos que $M_{0}$ es el *modelo nulo*, el cual contiene no predictores. Este modelo solamente predice la media de muestra para cada observación.
2) Para $k = 1, 2,..., p:$
    a) Ajustar todos los modelos $\binom{p}{k}$ que contengan el predictor *k*.
    b) Elegir el modelos entre todos los modelos $\binom{p}{k}$, a este lo llamaremos $M_{k}$. En este paso, el *mejor* modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el $R^2$ más grande.
3) En este último paso, seleccionamos el mejor modelos entre $M_{0},...,M_{p}$ usando validación cruzada para predecir el error, $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado.

Aunque la selección del mejor subconjunto es muy utilizada, usualmente es computalmente difícil de realizar si el número de predictores es muy alto. Cuando *p* es muy grande, mayor el espacio de búsqueda -esto puede sobre estimar los datos y aumentar la varianza de los coeficientes de los estimados-, más alta será la probabilidad de encontrar un model que se vea bien en los datos de entrenamiento, aunque puede que no tenga el mayor poder predictivo sobre los datos futuros.


### Selección paso a paso

Debido a las diferentes complicaciones que tiene la *selección del mejor subconjunto*, se tienen otro tipo de métodos que usan modelos más restrictivo, lo cual los hace alternativas muchas más atractivos que la *selección del mejor subconjunto*. Este método es conocido como *selección paso a paso*, dentro de este hay tres diferentes tipos: selección hacia adelante, selección hacia atrás y selección hídrida.

#### Selección hacia adelante

Este tipo de selección tiene el nombre de selección hacia a delanta, porque comienza con un modelo que no contiene ningún predictor al comienzo, pero a medida que el modelo está corriendo se añaden variables al modelo una a la vez, hasta que todos las predictores estén dentro del modelo. Particularmente, solo las variables que causen un *mejoramiento adicional* al ajuste del modelo. Este modelo funciona de la siguiente manera:

1) Supongamos que $M_{0}$ es el *modelo nulo*, el cual contiene no predictores.
2) Para $k = 0, 2,..., p - 1:$
    a) Ajustar todos los modelos $p - k$ que aumenten los predictores $M_{k}$ con un predictor adicional.
    b) Elegir el modelos entre todos los modelos $p - k$, a este lo llamaremos $M_{k + 1}$. En este paso, el *mejor* modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el $R^2$ más grande.
3) En este último paso, seleccionamos el mejor modelos entre $M_{0},...,M_{p}$ usando validación cruzada para predecir el error, $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado.

Selección hacia adelante tiene muchas ventajas con respecto a la selección del mejor subconjunto, sin embargo, a veces no garantiza encontrar el mejor modelo posible para todos los modelos $2^{p}$ que contiene el subconjunto de predictores *p*. Para explicar esto, supongamos que tenemos un conjunto de datos con $p = 3$ predicotres, donde el mejor modelo posible de una variable contiene $X_{1}$, y el mejor modelo con dos variables contiene $X_{2}$ y $X_{3}$. En este tipo de casos, la selección hacia adelante no eligirá el mejor modelo que contiene dos varibles, porque $M_{1}$ contendría $X_{1}$, así que $M_{2}$ tendría que contener $X_{1}$ con una variable adicional.

Asimismo, la selección hacia adelante puede usarse para problemas de alta dimensionalidad, donde $n < p$. Sin embargo, en este caso, es posible construir submodelos $M_{0},...,M_{n-1}$, dado que cada submodelos se ajusta usando mínimos cuadrados donde no va a haber una única respuesta si $p \geq n$. 

#### Selección hacia atrás

La selección hacia atrás es una alternativa eficiente para elegir el mejor subconjunto. Esta se diferencia de la selección hacia adelante porque comienza con un modelo de mínimos cuadrados que contiene todos los predictores *p*, y en cada iteración elimina el predictor que no esté aportando al modelo, uno a la vez. Este modelo funciona de la siguiente manera:

1) Supongamos que $M_{p}$ es el *modelo nulo*, el cual contiene todos los predictores *p*.
2) Para $k = p, p - 1,..., 1:$
    a) Considerar todos los modelos $k$ que contienen todas las variables menos uno en $M_{k}$, para un total de $k - 1$ predictores.
    b) Elegir el *mejor* entre todos los modelos $k$, a este lo llamaremos $M_{k - 1}$. En este paso, el *mejor* modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el $R^2$ más grande.
3) En este último paso, seleccionamos el mejor modelos entre $M_{0},...,M_{p}$ usando validación cruzada para predecir el error, $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado.

La selección hacia atrás buscan solamente $1 + p(p + 1)/2$ modelos, también puede ser aplicado cuando *p* es demasiado alto como para aplicar la selección del mejor subconjunto. No obstante, al igual que en la selección hacia adelante, la selección hacia atrás no garatiza encontrar el *mejor* modelo que contenga un subconjunto con *p* predictores. Uno de los requerimientos de la selección hacia atrás es que el número de muestras *n* sea mayor que el número de variables *p*.


#### Selección híbrida

La selección híbrida, como su nombre lo indica, es una versión híbrida entre la selección hacia adelante y la selección hacia atrás. En donde se añaden variables al modelo de manera seguida - como se hace en la selección adelanta -, pero luego de añadir cada nueva variable, la selección híbrida elimina las variables que no mejoren el ajuste del modelo. Este tipo de selección imita un poco a la *selección del mejor subconjunto*, pero a su vez, retiene las ventajas computacionales de la selección hacia adelana y hacia atrás.


### Selección del modelo óptimo

Para poder implementar cualquiera de los tres modelos que hablamos anteriormente, tenemos que determinar cuál de estos modelos es el *mejor*. Como ya hemos hablado anteriormente, el modelo que tenga todos los predictores va a tener el $RSS$ más pequeño y el $R^{2}$ más alto, dado que estos valores está relacionados con el error de entrenamiento. Sin embargo, el error de entrenamiento no es ideal para estimar el error de prueba. Por lo tanto, el $RSS$ y el $R^{2}$ no son ideales para seleccionar el mejor modelo para un grupo de modelos que tiene diferente número de predictores. 

Para poder elegir el mejor modelo con respecto al error de prueba, necesitamos estimar el error de prueba, para esto se pueden utilizar dos tipos de acercamiento:

1) Podemos estimar el error de prueba de manera indirecta por medio de ajustar el error de entrenamiento para tener en cuenta el sesgo causado por el sobreajuste.
2) También lo podemos estimar de manera directa usando un conjunto de validación o haciendo una validación cruzada.

#### $C_{p}$, $AIC$, $BIC$ y $R^{2}$ ajustado

Hay cuatro diferentes modelos que vamos mostrar en este librillo que son usado para la selección del cionjunto de modelos con diferentes números de variables. Estos cuatro modelos son: $C_{p}$, criterio de información de Akaike ($AIC$), criterio de información Bayesiano ($BIC$) y $R^{2}$ ajustado.

##### Mallow's $C_{p}$ 
Para un modelo ajustando de mínimos crucados que contine *d* predictores, el $C_{p}$ estima el MSE de prueba, el cual se computa usando la siguiente formula:
 
$$C_{p} = \frac{1}{n}(RSS + 2d\hat{\alpha}^{2})$$ 
Donde $\hat{\alpha}^{2})$ es el estimado del error de varianza $\epsilon$ asociado a cada medida de respuesta y *d* es el número total de parámetros usados. Este método añade una penalidad de $2d\hat{\alpha}^{2})$ para el RSS de entrenamiento, debido a que el error de entrenamiento tiene a subestimar el error de prueba. *Nota: esta penalidad incrementa a medida que el número de predictores en el modelo aumenta* 

##### Criterio de información de Akaike ($AIC$)

El AIC usualmente se utiliza para modelos ajustados con muchas clases usando máxima verosimilitud. 

$$AIC = -2 logL + 2 ⋅ d$$ 
Donde *L* es el valor maximizado de la función de la verosimilitus para el estimado del modelo. Para modelos de mínimos cuadrados, $C_{p}$ y AIC son proporsionales.


##### Criterio de información Bayesiano ($BIC$)

BIC es derivado desde un punto de vista Bayesiano, pero es algo parecido al $C_{p}$ y *AIC*. Para los modelos de mínimos cuadrados con *d* predictores, para hallar el valor de BIC se calcula utilizando la siguiente ecuación

$$BIC = \frac{1}{n}(RSS + log(n)d\hat{\alpha}^{2})$$ 

Donde *n* es el número de observaciones. Al igual que $C_{p}$, *BIC* va a tender a tener valores bajos para modelos con errores de prueba pequeños, y usualmente, nosotros seleccionamos el modelo que tenga el valor de BIC más bajo. Igualmente, podemos notar que el térmico $2d\hat{\alpha}^{2})$ en $C_{p}$ es reemplazado por $log(n)d\hat{\alpha}^{2}$ en BIC. Esto indica que cuando log$n > 2$ para cualquier $n > 7$, el estadístico BIC da una mayor penalidad a modelos que tienen muchas variables, por lo tanto, tiende a selecciones modelos más pequeños que $C_{p}$.

##### $R^{2}$ ajustado

El estadístico $R^{2}$ es uno de los métodos más utilizados para seleccionar un modelo en particular entre varios modelos que tienen diferente número de variables. Para un modelos de mínimos cuadrados con *d* variables, el $R^{2}$ ajustado es calculado por medio de la siguiente formula

$$R^{2} ajustado = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$$
A diferencia de $C_{p}$, AIC y BIC que toman el mejor modelo que tengan en *menor* valor de error de prueba, un valor de $R^{2}$ ajustado *grande* indica el modelo con el error de prueba más bajo. Para el caso de $R^{2}$ ajustado es equivalente a maximizar $\frac{RSS}{n-d-1}$. Mientras que RSS decrece a medida que el número de variables aumenta, $\frac{RSS}{n-d-1}$ puede que incremente o disminuye debido a la presencia de *d* en el denominador. A diferencia del estadístico $R^{2}$, el estadístico $R^{2}$ ajustado  *paga un precio* por incluir variables innecesarias en el modelo.

#### Validación y validación cruzada

Otra alternativa para los métodos que vimos anteriormente, donde podemos estimar directamente el error de prueba es usando un conjunto de validación o la validación cruzada. Con cualquiera de estos dos métodos podemos calcular el error para cada uno de los modelos, y luego seleccionar el mejor modelo para el cual el estimado para el error de prueba es el más pequeño. 

Cuando se hace el conjunto de validación o la validación cruzada se tienen dos ventajas a comparación de $C_{p}$, $AIC$, $BIC$ o $R^{2}$ ajustado, ambos tipos de validación calculan directamente el error de pruba y no requiere estimar el error de varianza, $\sigma^{2}$. Del mismo modo, la validación cruzada puede ser usada para un mayor número de tareas de selecció de modelo, incluso cuando es difícil de estimar los *grados de libertad* o el error de varianza, $\sigma^{2}$.

Para llevar a acabo ambos tipos de validación necesitamos que cada uno de los procedimientos retorne a una secuencia de modelos $M_{k}$ para modelos con un tamaño de $k = 0, 1, 2...$. Nuestro trabajo es seleccionar un $\hat{k}$. Una vez seleccioando, vamos a retornar a $M_{k}$. Necesitamos computar el error de conjunto de validación o el error de validación cruzada para cada modelo $M_{k}$ que se encuentra bajo consideració, y luego, selecionar el *k* para cada estimado donde el error de prueba sea el más pequeño.

## Métodos de contracción (Cristian)

### Regresión de crestas (ridge regression)

#### ¿Por qué la regresión de crestas es *mejor* que los mínimos cuadrados?

### El Lasso

#### Otra formulación para regresión de crestas y El Lasso

#### Propiedad de selección de variables de The Lasso

#### Comparación entre El Lasso y la regresión de crestas

####  Un caso especies para la regresión de crestas y El Lasso

#### Interpretación Bayesiana para la regresión de crestas y El Lasso

### Selección de ajuste de parámetros