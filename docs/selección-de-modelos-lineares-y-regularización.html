<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitulo 6 Selección de modelos lineares y regularización | Introducción al uso de machine learning en biología</title>
  <meta name="description" content="Capitulo 6 Selección de modelos lineares y regularización | Introducción al uso de machine learning en biología" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitulo 6 Selección de modelos lineares y regularización | Introducción al uso de machine learning en biología" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitulo 6 Selección de modelos lineares y regularización | Introducción al uso de machine learning en biología" />
  
  
  

<meta name="author" content="Jhan C. Salazar y Cristian Román-Palacios" />


<meta name="date" content="2023-09-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="métodos-de-remuestreo.html"/>
<link rel="next" href="métodos-basados-en-árboles-jhan.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ML y Biologia</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Una introducción breve a Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html"><i class="fa fa-check"></i><b>2</b> ¿Qué es Machine Learning?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#tipos-de-machine-learning"><i class="fa fa-check"></i><b>2.1</b> Tipos de machine learning</a></li>
<li class="chapter" data-level="2.2" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#definiciones-relevantes-a-machine-learning"><i class="fa fa-check"></i><b>2.2</b> Definiciones relevantes a machine learning</a></li>
<li class="chapter" data-level="2.3" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#tipos-de-particiones-de-datos-en-machine-learning"><i class="fa fa-check"></i><b>2.3</b> Tipos de particiones de datos en machine learning</a></li>
<li class="chapter" data-level="2.4" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#introducción-a-los-artículos-y-datasets-que-se-usarán-en-este-librillo"><i class="fa fa-check"></i><b>2.4</b> Introducción a los artículos y datasets que se usarán en este librillo</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html"><i class="fa fa-check"></i><b>3</b> Capítulo 3: Regresiones lineales</a>
<ul>
<li class="chapter" data-level="3.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#regresión-lineal-univariada"><i class="fa fa-check"></i><b>3.1</b> Regresión lineal univariada</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-lineales-univariadas-para-los-coeficientes-y-el-modelo"><i class="fa fa-check"></i><b>3.1.1</b> Mediciones de error para regresiones lineales univariadas para los coeficientes y el modelo</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#regresión-lineal-de-multiples-variables"><i class="fa fa-check"></i><b>3.2</b> Regresión lineal de multiples variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-de-multiples-variables-para-los-coeficientes-y-el-modelo"><i class="fa fa-check"></i><b>3.2.1</b> Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo</a></li>
<li class="chapter" data-level="3.2.2" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-de-multiples-variables-para-los-coeficientes-y-el-modelo-1"><i class="fa fa-check"></i><b>3.2.2</b> Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo</a></li>
<li class="chapter" data-level="3.2.3" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#interacciones-entre-variables"><i class="fa fa-check"></i><b>3.2.3</b> Interacciones entre variables</a></li>
<li class="chapter" data-level="3.2.4" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#comparaciones-y-decisiones-entre-modelos-de-multiples-variables"><i class="fa fa-check"></i><b>3.2.4</b> Comparaciones y decisiones entre modelos de multiples variables</a></li>
<li class="chapter" data-level="3.2.5" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#asunsiones-y-posibles-problemas-de-las-regresiones-lineales-y-cómo-lidiar-con-ellas"><i class="fa fa-check"></i><b>3.2.5</b> Asunsiones y posibles problemas de las regresiones lineales y cómo lidiar con ellas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clasificaciones.html"><a href="clasificaciones.html"><i class="fa fa-check"></i><b>4</b> Clasificaciones</a>
<ul>
<li class="chapter" data-level="4.1" data-path="clasificaciones.html"><a href="clasificaciones.html#regresión-logística"><i class="fa fa-check"></i><b>4.1</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="clasificaciones.html"><a href="clasificaciones.html#estimación-de-coeficiente-en-regresiones-logísticas"><i class="fa fa-check"></i><b>4.1.1</b> Estimación de coeficiente en regresiones logísticas</a></li>
<li class="chapter" data-level="4.1.2" data-path="clasificaciones.html"><a href="clasificaciones.html#regresiones-logísticas-múltiples-y-para-2-clases-de-respuestas"><i class="fa fa-check"></i><b>4.1.2</b> Regresiones logísticas múltiples y para &gt;2 clases de respuestas</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>4.2</b> Análisis discriminante lineal</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="clasificaciones.html"><a href="clasificaciones.html#teorema-de-bayes-para-las-clasificaciones"><i class="fa fa-check"></i><b>4.2.1</b> Teorema de Bayes para las clasificaciones</a></li>
<li class="chapter" data-level="4.2.2" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1"><i class="fa fa-check"></i><b>4.2.2</b> Análisis discriminante lineal para <span class="math inline">\(p = 1\)</span></a></li>
<li class="chapter" data-level="4.2.3" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1-1"><i class="fa fa-check"></i><b>4.2.3</b> Análisis discriminante lineal para <span class="math inline">\(p &gt; 1\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discrimimante-cuadrático"><i class="fa fa-check"></i><b>4.2.4</b> Análisis discrimimante cuadrático</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="clasificaciones.html"><a href="clasificaciones.html#clasificador-bayesiano-ingenuo"><i class="fa fa-check"></i><b>4.3</b> Clasificador bayesiano ingenuo</a></li>
<li class="chapter" data-level="4.4" data-path="clasificaciones.html"><a href="clasificaciones.html#k-vecinos-más-cercanos"><i class="fa fa-check"></i><b>4.4</b> K-vecinos más cercanos</a></li>
<li class="chapter" data-level="4.5" data-path="clasificaciones.html"><a href="clasificaciones.html#comparación-de-métodos-de-clasificación"><i class="fa fa-check"></i><b>4.5</b> Comparación de métodos de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html"><i class="fa fa-check"></i><b>5</b> Métodos de remuestreo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada"><i class="fa fa-check"></i><b>5.1</b> Validación cruzada</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#conjunto-de-validación"><i class="fa fa-check"></i><b>5.1.1</b> Conjunto de validación</a></li>
<li class="chapter" data-level="5.1.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-dejando-un-elemento-fuera-loocv"><i class="fa fa-check"></i><b>5.1.2</b> Validación cruzada dejando un elemento fuera (LOOCV)</a></li>
<li class="chapter" data-level="5.1.3" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-de-k-iteraciones-k-fold-cv"><i class="fa fa-check"></i><b>5.1.3</b> Validación cruzada de K-iteraciones (<em>k</em>-fold CV)</a></li>
<li class="chapter" data-level="5.1.4" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#sesgo-y-varianza-para-validación-cruzada-de-k-interaciones"><i class="fa fa-check"></i><b>5.1.4</b> Sesgo y varianza para validación cruzada de K-interaciones</a></li>
<li class="chapter" data-level="5.1.5" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-en-problemas-de-clasificación"><i class="fa fa-check"></i><b>5.1.5</b> Validación cruzada en problemas de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#boopstrap-o-arranque"><i class="fa fa-check"></i><b>5.2</b> Boopstrap (o arranque)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#otros-usos-del-bootstrap"><i class="fa fa-check"></i><b>5.2.1</b> Otros usos del bootstrap</a></li>
<li class="chapter" data-level="5.2.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#pre-validación"><i class="fa fa-check"></i><b>5.2.2</b> Pre-validación</a></li>
<li class="chapter" data-level="5.2.3" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#bootstrap-versus-permutaciones"><i class="fa fa-check"></i><b>5.2.3</b> Bootstrap versus permutaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html"><i class="fa fa-check"></i><b>6</b> Selección de modelos lineares y regularización</a>
<ul>
<li class="chapter" data-level="6.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-de-subconjuntos"><i class="fa fa-check"></i><b>6.1</b> Selección de subconjuntos</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-del-mejor-subconjunto"><i class="fa fa-check"></i><b>6.1.1</b> Selección del mejor subconjunto</a></li>
<li class="chapter" data-level="6.1.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-paso-a-paso"><i class="fa fa-check"></i><b>6.1.2</b> Selección paso a paso</a></li>
<li class="chapter" data-level="6.1.3" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-del-modelo-óptimo"><i class="fa fa-check"></i><b>6.1.3</b> Selección del modelo óptimo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#métodos-de-contracción-cristian"><i class="fa fa-check"></i><b>6.2</b> Métodos de contracción (Cristian)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#regresión-de-crestas-ridge-regression"><i class="fa fa-check"></i><b>6.2.1</b> Regresión de crestas (ridge regression)</a></li>
<li class="chapter" data-level="6.2.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#el-lasso"><i class="fa fa-check"></i><b>6.2.2</b> El Lasso</a></li>
<li class="chapter" data-level="6.2.3" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-de-ajuste-de-parámetros"><i class="fa fa-check"></i><b>6.2.3</b> Selección de ajuste de parámetros</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="métodos-basados-en-árboles-jhan.html"><a href="métodos-basados-en-árboles-jhan.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles (Jhan)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="métodos-basados-en-árboles-jhan.html"><a href="métodos-basados-en-árboles-jhan.html#decision-trees"><i class="fa fa-check"></i><b>7.1</b> Decision trees</a></li>
<li class="chapter" data-level="7.2" data-path="métodos-basados-en-árboles-jhan.html"><a href="métodos-basados-en-árboles-jhan.html#boosting-and-bagging"><i class="fa fa-check"></i><b>7.2</b> Boosting and bagging</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="métodos-basados-en-árboles-jhan.html"><a href="métodos-basados-en-árboles-jhan.html#lightgbm-cristian"><i class="fa fa-check"></i><b>7.2.1</b> lightGBM (Cristian)</a></li>
<li class="chapter" data-level="7.2.2" data-path="métodos-basados-en-árboles-jhan.html"><a href="métodos-basados-en-árboles-jhan.html#xgboost-cristian"><i class="fa fa-check"></i><b>7.2.2</b> XGboost (Cristian)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al uso de machine learning en biología</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="selección-de-modelos-lineares-y-regularización" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Capitulo 6</span> Selección de modelos lineares y regularización<a href="selección-de-modelos-lineares-y-regularización.html#selección-de-modelos-lineares-y-regularización" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>En este capítulos vamos a extender lo que sabemos hasta ahora de modelos lineales. En otras palabras, vamos ahabalr sobre cómmo podemos mejorar los modelos lineales simples, por medio de reemplazar los ajuste de mínimos caudrados con procedimientos alternativos de ajuste. Lo cual hace que haya una mejor la <em>precisión de la predicción</em> y la <em>interpretabilidad del modelo</em>.</p>
<ul>
<li><p><em>Precisión de la predicción</em>: Esto provee la relación verdadera entre la respuesta y los predictores es aproximadamente lineal, donde los estimados de mínimos cuadrados tendrían un bajo sesgo. Cuando el número de observaciones <em>n</em> es mayor al número de variables <em>p</em>, entonces los estimados de mínimos cuadrados van a tender a tener baja varianza, lo cual causa que este sea efectivo en observaciones de prueba. Mientras que cuando <em>n</em> no es mucho más grande que <em>p</em>, entonces hay una alta variablilldad en el ajuste de los mínimos cuadrados, lo cual resulta en una sobreestimación y baja predictabilidad en observaciones futuras que no fueron utilizadas en el modelo. Cuando el número de predictores es mayor que el número de observaciones, no hay un solo estimado del coeficiente de mínimos cuadrados, es decir, la varianza es infinita, lo cual no permite que se puede usar este método. Por medio de <em>restringir</em> o <em>contraer</em> los coeficientes estimados podemos reducir la varianza a costo de aumentar el sesgo. Esto puede dar como resultados el mejorar al precisión, lo cual permite predecir la respoestado de observaciones que no fueron usadas en el modelo de entrenamiento.</p></li>
<li><p><em>Interpretabilidad del modelo</em>: Esto se refiere a eliminar variables <em>innecesarias</em> que solo le agregan complejidad al modelo -por medio de ajustar los coeficientes de los estimados a cero. De esta manera, el modelo se vuelvo muchos mas interpretable. No obstante, hay que tener en cuenta que en los mínimos cuadrados es poco pobable obtener estimados de coeficientes que den exactamente cero.</p></li>
</ul>
<p>Hay diferentes alternativas para ajustar el uso de los mínimos cuadrados.</p>
<ul>
<li><p><em>Selección de subconjunto</em>: Este procedimiento trata de identificar un subconjunto de <em>p</em> predictores que se cree que están relacionados con la respuesta, para luego ajustar el modelo usado en los mínimos caudrados para reducir el conjunto de variables.</p></li>
<li><p><em>Contracción</em>: Este procedimiento requiere ajustar un modelo con todos <em>p</em> predictores. Lo que hace ue haya un efecto en la reducción de la varianza, a su vez, de pendiendo del tipo de contracción que se utilice algunos de los coeficientes puede ser estimados como cero, lo que hace que el método de contracción pueda usarse para la selección de variables.</p></li>
<li><p><em>Reducción dimensiones</em>: Este método tiene como objetivo <em>proyectar</em> los predictores <em>p</em> en <em>M</em> dimensiones subespaciales donde <span class="math inline">\(M &lt; p\)</span>. Lo cual se logra por medio de computar <em>M</em> <em>combinaciones lineares</em> diferentes o <em>proyecciones</em> de las variables. Luego, esta <em>M</em> proyecciones se usan como predictores para ajustar el modelos de regresión lineal por medio de mínimos cuadrados.</p></li>
</ul>
<div id="selección-de-subconjuntos" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Selección de subconjuntos<a href="selección-de-modelos-lineares-y-regularización.html#selección-de-subconjuntos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En esta sección vamos a hablar de dos tipos de selección de subconjuntos: selección del mejor subconjunto y selección paso a paso.</p>
<div id="selección-del-mejor-subconjunto" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Selección del mejor subconjunto<a href="selección-de-modelos-lineares-y-regularización.html#selección-del-mejor-subconjunto" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para llevar a cabo este método, debemos ajustar separadamente regresiones de mínimos caudrados para cada combinación de predictores <em>p</em>. Es decir, ajustar modelos que tenga un predictor, luego para todos los modelos que contengan dos predictores <span class="math inline">\(\binom{p}{2} = p(p-1)/2\)</span>, y así sucesivamente. Luego, revisamos todos los modelos resultantes para identificar el mejor modelo. No obstante, debemos tener en cuenta que este tipo de selección puede ser problemático porque pueden haber <span class="math inline">\(2^p\)</span> posibilidades que debemos considerar para poder elegir el mejor subconjunto.</p>
<p>Para llevar a cabo la selección del mejor subconjunto necesitamos seguir los siguientes pasos:</p>
<ol style="list-style-type: decimal">
<li>Supongamos que <span class="math inline">\(M_{0}\)</span> es el <em>modelo nulo</em>, el cual contiene no predictores. Este modelo solamente predice la media de muestra para cada observación.</li>
<li>Para <span class="math inline">\(k = 1, 2,..., p:\)</span>
<ol style="list-style-type: lower-alpha">
<li>Ajustar todos los modelos <span class="math inline">\(\binom{p}{k}\)</span> que contengan el predictor <em>k</em>.</li>
<li>Elegir el modelos entre todos los modelos <span class="math inline">\(\binom{p}{k}\)</span>, a este lo llamaremos <span class="math inline">\(M_{k}\)</span>. En este paso, el <em>mejor</em> modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el <span class="math inline">\(R^2\)</span> más grande.</li>
</ol></li>
<li>En este último paso, seleccionamos el mejor modelos entre <span class="math inline">\(M_{0},...,M_{p}\)</span> usando validación cruzada para predecir el error, <span class="math inline">\(C_{p}\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> o <span class="math inline">\(R^{2}\)</span> ajustado.</li>
</ol>
<p>Aunque la selección del mejor subconjunto es muy utilizada, usualmente es computalmente difícil de realizar si el número de predictores es muy alto. Cuando <em>p</em> es muy grande, mayor el espacio de búsqueda -esto puede sobre estimar los datos y aumentar la varianza de los coeficientes de los estimados-, más alta será la probabilidad de encontrar un model que se vea bien en los datos de entrenamiento, aunque puede que no tenga el mayor poder predictivo sobre los datos futuros.</p>
</div>
<div id="selección-paso-a-paso" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Selección paso a paso<a href="selección-de-modelos-lineares-y-regularización.html#selección-paso-a-paso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Debido a las diferentes complicaciones que tiene la <em>selección del mejor subconjunto</em>, se tienen otro tipo de métodos que usan modelos más restrictivo, lo cual los hace alternativas muchas más atractivos que la <em>selección del mejor subconjunto</em>. Este método es conocido como <em>selección paso a paso</em>, dentro de este hay tres diferentes tipos: selección hacia adelante, selección hacia atrás y selección hídrida.</p>
<div id="selección-hacia-adelante" class="section level4 hasAnchor" number="6.1.2.1">
<h4><span class="header-section-number">6.1.2.1</span> Selección hacia adelante<a href="selección-de-modelos-lineares-y-regularización.html#selección-hacia-adelante" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Este tipo de selección tiene el nombre de selección hacia a delanta, porque comienza con un modelo que no contiene ningún predictor al comienzo, pero a medida que el modelo está corriendo se añaden variables al modelo una a la vez, hasta que todos las predictores estén dentro del modelo. Particularmente, solo las variables que causen un <em>mejoramiento adicional</em> al ajuste del modelo. este modelo funciona de la siguiente manera:</p>
<ol style="list-style-type: decimal">
<li>Supongamos que <span class="math inline">\(M_{0}\)</span> es el <em>modelo nulo</em>, el cual contiene no predictores.</li>
<li>Para <span class="math inline">\(k = 0, 2,..., p - 1:\)</span>
<ol style="list-style-type: lower-alpha">
<li>Ajustar todos los modelos <span class="math inline">\(p - k\)</span> que aumenten los predictores <span class="math inline">\(M_{k}\)</span> con un predictor adicional.</li>
<li>Elegir el modelos entre todos los modelos <span class="math inline">\(p - k\)</span>, a este lo llamaremos <span class="math inline">\(M_{k + 1}\)</span>. En este paso, el <em>mejor</em> modelo se puede identificar porque tiene el RSS más pequeño o, equivalentemente, el <span class="math inline">\(R^2\)</span> más grande.</li>
</ol></li>
<li>En este último paso, seleccionamos el mejor modelos entre <span class="math inline">\(M_{0},...,M_{p}\)</span> usando validación cruzada para predecir el error, <span class="math inline">\(C_{p}\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> o <span class="math inline">\(R^{2}\)</span> ajustado.</li>
</ol>
<p>Selección hacia adelante tiene muchas ventajas con respecto a la selección del mejor subconjunto, sin embargo, a veces no garantiza encontrar el mejor modelo posible paara todos los moldes <span class="math inline">\(2^{p}\)</span> que contiene el subconjunto de predictores <em>p</em>. Para explicar esto, supongamos que tenemos un conjunto de datos con <span class="math inline">\(p = 3\)</span> predicotres, donde el mejor modelo posible de una variable contiene <span class="math inline">\(X_{1}\)</span></p>
</div>
<div id="selección-hacia-atrás" class="section level4 hasAnchor" number="6.1.2.2">
<h4><span class="header-section-number">6.1.2.2</span> Selección hacia atrás<a href="selección-de-modelos-lineares-y-regularización.html#selección-hacia-atrás" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="selección-híbrida" class="section level4 hasAnchor" number="6.1.2.3">
<h4><span class="header-section-number">6.1.2.3</span> Selección híbrida<a href="selección-de-modelos-lineares-y-regularización.html#selección-híbrida" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
<div id="selección-del-modelo-óptimo" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Selección del modelo óptimo<a href="selección-de-modelos-lineares-y-regularización.html#selección-del-modelo-óptimo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="c_p-aic-bic-y-r2-ajustado" class="section level4 hasAnchor" number="6.1.3.1">
<h4><span class="header-section-number">6.1.3.1</span> <span class="math inline">\(C_{p}\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> y <span class="math inline">\(R^{2}\)</span> ajustado<a href="selección-de-modelos-lineares-y-regularización.html#c_p-aic-bic-y-r2-ajustado" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="validación-y-validación-cruzada" class="section level4 hasAnchor" number="6.1.3.2">
<h4><span class="header-section-number">6.1.3.2</span> Validación y validación cruzada<a href="selección-de-modelos-lineares-y-regularización.html#validación-y-validación-cruzada" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
</div>
<div id="métodos-de-contracción-cristian" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Métodos de contracción (Cristian)<a href="selección-de-modelos-lineares-y-regularización.html#métodos-de-contracción-cristian" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="regresión-de-crestas-ridge-regression" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Regresión de crestas (ridge regression)<a href="selección-de-modelos-lineares-y-regularización.html#regresión-de-crestas-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="por-qué-la-regresión-de-crestas-es-mejor-que-los-mínimos-cuadrados" class="section level4 hasAnchor" number="6.2.1.1">
<h4><span class="header-section-number">6.2.1.1</span> ¿Por qué la regresión de crestas es <em>mejor</em> que los mínimos cuadrados?<a href="selección-de-modelos-lineares-y-regularización.html#por-qué-la-regresión-de-crestas-es-mejor-que-los-mínimos-cuadrados" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
<div id="el-lasso" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> El Lasso<a href="selección-de-modelos-lineares-y-regularización.html#el-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="otra-formulación-para-regresión-de-crestas-y-el-lasso" class="section level4 hasAnchor" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> Otra formulación para regresión de crestas y El Lasso<a href="selección-de-modelos-lineares-y-regularización.html#otra-formulación-para-regresión-de-crestas-y-el-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="propiedad-de-selección-de-variables-de-the-lasso" class="section level4 hasAnchor" number="6.2.2.2">
<h4><span class="header-section-number">6.2.2.2</span> Propiedad de selección de variables de The Lasso<a href="selección-de-modelos-lineares-y-regularización.html#propiedad-de-selección-de-variables-de-the-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="comparación-entre-el-lasso-y-la-regresión-de-crestas" class="section level4 hasAnchor" number="6.2.2.3">
<h4><span class="header-section-number">6.2.2.3</span> Comparación entre El Lasso y la regresión de crestas<a href="selección-de-modelos-lineares-y-regularización.html#comparación-entre-el-lasso-y-la-regresión-de-crestas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="un-caso-especies-para-la-regresión-de-crestas-y-el-lasso" class="section level4 hasAnchor" number="6.2.2.4">
<h4><span class="header-section-number">6.2.2.4</span> Un caso especies para la regresión de crestas y El Lasso<a href="selección-de-modelos-lineares-y-regularización.html#un-caso-especies-para-la-regresión-de-crestas-y-el-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="interpretación-bayesiana-para-la-regresión-de-crestas-y-el-lasso" class="section level4 hasAnchor" number="6.2.2.5">
<h4><span class="header-section-number">6.2.2.5</span> Interpretación Bayesiana para la regresión de crestas y El Lasso<a href="selección-de-modelos-lineares-y-regularización.html#interpretación-bayesiana-para-la-regresión-de-crestas-y-el-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
<div id="selección-de-ajuste-de-parámetros" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Selección de ajuste de parámetros<a href="selección-de-modelos-lineares-y-regularización.html#selección-de-ajuste-de-parámetros" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="métodos-de-remuestreo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="métodos-basados-en-árboles-jhan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/RomanPalacios-lab/ml-biologia/edit/main/06-lmsreg.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/RomanPalacios-lab/ml-biologia/blob/main/06-lmsreg.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
