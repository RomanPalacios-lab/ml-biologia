<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitulo 4 Clasificaciones | Introducción al uso de machine learning en biología</title>
  <meta name="description" content="Capitulo 4 Clasificaciones | Introducción al uso de machine learning en biología" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitulo 4 Clasificaciones | Introducción al uso de machine learning en biología" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitulo 4 Clasificaciones | Introducción al uso de machine learning en biología" />
  
  
  

<meta name="author" content="Jhan C. Salazar y Cristian Román-Palacios" />


<meta name="date" content="2023-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="capítulo-3-regresiones-lineales.html"/>
<link rel="next" href="métodos-de-remuestreo.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ML y Biologia</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Una introducción breve a Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html"><i class="fa fa-check"></i><b>2</b> ¿Qué es Machine Learning?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#tipos-de-machine-learning"><i class="fa fa-check"></i><b>2.1</b> Tipos de machine learning</a></li>
<li class="chapter" data-level="2.2" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#definiciones-relevantes-a-machine-learning"><i class="fa fa-check"></i><b>2.2</b> Definiciones relevantes a machine learning</a></li>
<li class="chapter" data-level="2.3" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#tipos-de-particiones-de-datos-en-machine-learning"><i class="fa fa-check"></i><b>2.3</b> Tipos de particiones de datos en machine learning</a></li>
<li class="chapter" data-level="2.4" data-path="qué-es-machine-learning.html"><a href="qué-es-machine-learning.html#introducción-a-los-artículos-y-datasets-que-se-usarán-en-este-librillo"><i class="fa fa-check"></i><b>2.4</b> Introducción a los artículos y datasets que se usarán en este librillo</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html"><i class="fa fa-check"></i><b>3</b> Capítulo 3: Regresiones lineales</a>
<ul>
<li class="chapter" data-level="3.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#regresión-lineal-univariada"><i class="fa fa-check"></i><b>3.1</b> Regresión lineal univariada</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-lineales-univariadas-para-los-coeficientes-y-el-modelo"><i class="fa fa-check"></i><b>3.1.1</b> Mediciones de error para regresiones lineales univariadas para los coeficientes y el modelo</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#regresión-lineal-de-multiples-variables"><i class="fa fa-check"></i><b>3.2</b> Regresión lineal de multiples variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-de-multiples-variables-para-los-coeficientes-y-el-modelo"><i class="fa fa-check"></i><b>3.2.1</b> Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo</a></li>
<li class="chapter" data-level="3.2.2" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#mediciones-de-error-para-regresiones-de-multiples-variables-para-los-coeficientes-y-el-modelo-1"><i class="fa fa-check"></i><b>3.2.2</b> Mediciones de error para regresiones de multiples variables para los coeficientes y el modelo</a></li>
<li class="chapter" data-level="3.2.3" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#interacciones-entre-variables"><i class="fa fa-check"></i><b>3.2.3</b> Interacciones entre variables</a></li>
<li class="chapter" data-level="3.2.4" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#comparaciones-y-decisiones-entre-modelos-de-multiples-variables"><i class="fa fa-check"></i><b>3.2.4</b> Comparaciones y decisiones entre modelos de multiples variables</a></li>
<li class="chapter" data-level="3.2.5" data-path="capítulo-3-regresiones-lineales.html"><a href="capítulo-3-regresiones-lineales.html#asunsiones-y-posibles-problemas-de-las-regresiones-lineales-y-cómo-lidiar-con-ellas"><i class="fa fa-check"></i><b>3.2.5</b> Asunsiones y posibles problemas de las regresiones lineales y cómo lidiar con ellas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clasificaciones.html"><a href="clasificaciones.html"><i class="fa fa-check"></i><b>4</b> Clasificaciones</a>
<ul>
<li class="chapter" data-level="4.1" data-path="clasificaciones.html"><a href="clasificaciones.html#regresión-logística"><i class="fa fa-check"></i><b>4.1</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="clasificaciones.html"><a href="clasificaciones.html#estimación-de-coeficiente-en-regresiones-logísticas"><i class="fa fa-check"></i><b>4.1.1</b> Estimación de coeficiente en regresiones logísticas</a></li>
<li class="chapter" data-level="4.1.2" data-path="clasificaciones.html"><a href="clasificaciones.html#regresiones-logísticas-múltiples-y-para-2-clases-de-respuestas"><i class="fa fa-check"></i><b>4.1.2</b> Regresiones logísticas múltiples y para &gt;2 clases de respuestas</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>4.2</b> Análisis discriminante lineal</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="clasificaciones.html"><a href="clasificaciones.html#teorema-de-bayes-para-las-clasificaciones"><i class="fa fa-check"></i><b>4.2.1</b> Teorema de Bayes para las clasificaciones</a></li>
<li class="chapter" data-level="4.2.2" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1"><i class="fa fa-check"></i><b>4.2.2</b> Análisis discriminante lineal para <span class="math inline">\(p = 1\)</span></a></li>
<li class="chapter" data-level="4.2.3" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1-1"><i class="fa fa-check"></i><b>4.2.3</b> Análisis discriminante lineal para <span class="math inline">\(p &gt; 1\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="clasificaciones.html"><a href="clasificaciones.html#análisis-discrimimante-cuadrático"><i class="fa fa-check"></i><b>4.2.4</b> Análisis discrimimante cuadrático</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="clasificaciones.html"><a href="clasificaciones.html#clasificador-bayesiano-ingenuo"><i class="fa fa-check"></i><b>4.3</b> Clasificador bayesiano ingenuo</a></li>
<li class="chapter" data-level="4.4" data-path="clasificaciones.html"><a href="clasificaciones.html#k-vecinos-más-cercanos"><i class="fa fa-check"></i><b>4.4</b> K-vecinos más cercanos</a></li>
<li class="chapter" data-level="4.5" data-path="clasificaciones.html"><a href="clasificaciones.html#comparación-de-métodos-de-clasificación"><i class="fa fa-check"></i><b>4.5</b> Comparación de métodos de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html"><i class="fa fa-check"></i><b>5</b> Métodos de remuestreo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada"><i class="fa fa-check"></i><b>5.1</b> Validación cruzada</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#conjunto-de-validación"><i class="fa fa-check"></i><b>5.1.1</b> Conjunto de validación</a></li>
<li class="chapter" data-level="5.1.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-dejando-un-elemento-fuera-loocv"><i class="fa fa-check"></i><b>5.1.2</b> Validación cruzada dejando un elemento fuera (LOOCV)</a></li>
<li class="chapter" data-level="5.1.3" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-de-k-iteraciones-k-fold-cv"><i class="fa fa-check"></i><b>5.1.3</b> Validación cruzada de K-iteraciones (<em>k</em>-fold CV)</a></li>
<li class="chapter" data-level="5.1.4" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#sesgo-y-varianza-para-validación-cruzada-de-k-interaciones"><i class="fa fa-check"></i><b>5.1.4</b> Sesgo y varianza para validación cruzada de K-interaciones</a></li>
<li class="chapter" data-level="5.1.5" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#validación-cruzada-en-problemas-de-clasificación"><i class="fa fa-check"></i><b>5.1.5</b> Validación cruzada en problemas de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#boopstrap-o-arranque"><i class="fa fa-check"></i><b>5.2</b> Boopstrap (o arranque)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#otros-usos-del-bootstrap"><i class="fa fa-check"></i><b>5.2.1</b> Otros usos del bootstrap</a></li>
<li class="chapter" data-level="5.2.2" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#pre-validación"><i class="fa fa-check"></i><b>5.2.2</b> Pre-validación</a></li>
<li class="chapter" data-level="5.2.3" data-path="métodos-de-remuestreo.html"><a href="métodos-de-remuestreo.html#bootstrap-versus-permutaciones"><i class="fa fa-check"></i><b>5.2.3</b> Bootstrap versus permutaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html"><i class="fa fa-check"></i><b>6</b> Selección de modelos lineares y regularización</a>
<ul>
<li class="chapter" data-level="6.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-de-subconjuntos"><i class="fa fa-check"></i><b>6.1</b> Selección de subconjuntos</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-del-mejor-subconjunto"><i class="fa fa-check"></i><b>6.1.1</b> Selección del mejor subconjunto</a></li>
<li class="chapter" data-level="6.1.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-paso-a-paso"><i class="fa fa-check"></i><b>6.1.2</b> Selección paso a paso</a></li>
<li class="chapter" data-level="6.1.3" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#selección-del-modelo-óptimo"><i class="fa fa-check"></i><b>6.1.3</b> Selección del modelo óptimo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#métodos-de-contracción"><i class="fa fa-check"></i><b>6.2</b> Métodos de contracción</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#regresión-de-crestas"><i class="fa fa-check"></i><b>6.2.1</b> Regresión de crestas</a></li>
<li class="chapter" data-level="6.2.2" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#section"><i class="fa fa-check"></i><b>6.2.2</b> </a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#métodos-de-reducción-dimensiones"><i class="fa fa-check"></i><b>6.3</b> Métodos de reducción dimensiones</a></li>
<li class="chapter" data-level="6.4" data-path="selección-de-modelos-lineares-y-regularización.html"><a href="selección-de-modelos-lineares-y-regularización.html#consideraciones-en-dimensiones-altas"><i class="fa fa-check"></i><b>6.4</b> Consideraciones en dimensiones altas</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="métodos-basados-en-árboles.html"><a href="métodos-basados-en-árboles.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles</a>
<ul>
<li class="chapter" data-level="7.1" data-path="métodos-basados-en-árboles.html"><a href="métodos-basados-en-árboles.html#lightgbm"><i class="fa fa-check"></i><b>7.1</b> lightGBM</a></li>
<li class="chapter" data-level="7.2" data-path="métodos-basados-en-árboles.html"><a href="métodos-basados-en-árboles.html#xgboost"><i class="fa fa-check"></i><b>7.2</b> XGboost</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al uso de machine learning en biología</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clasificaciones" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Capitulo 4</span> Clasificaciones<a href="clasificaciones.html#clasificaciones" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>En muchas situaciones de la vida real, la variable respuesta es cualitativa en lugar de cuantitativa, también llamadas variables categóricas. Las <strong>clasificaciones</strong> son la manera por la cual se pueden predecir la respuesta de las variables cualitativas. Hay que tener un par de cosas en cuenta con respecto a la clasificación. La primera es que <em>clasificar</em> se refiere a predecir la respuesta de una variable cuantitativa para una observación, es decir, estamos <em>clasificando</em> esa observación a la cual se le asigna una categoría o clase, y la segunda, los métodos de clasificación predicen la probabilidad de cada categoría de una variable cualitativa. En este capitulo vamos a discutir tres métodos de clasificación: regresión logística, análisis discriminante lineal y el k-vecinos más cercanos.</p>
<p>Independientemente de cuál de estos tres métodos utilicemos, para las clasificaciones también tenemos un set de observaciones de entrenamientos <span class="math inline">\((x_{1}, y_{1})\)</span>,…,<span class="math inline">\((x_{n}, y_{n})\)</span>, las cuales se usan para construir el clasificador. Lo que buscamos es que nuestro clasificador sea bueno para clasificar los datos de entrenamiento, al igual que en las observaciones de prueba que no fueron usadas para entrenar el clasificador.</p>
<div id="regresión-logística" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Regresión logística<a href="clasificaciones.html#regresión-logística" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Los modelos de regresión logística modelas la <strong>probabilidad</strong> de que <em>Y</em> pertenezca a una categoría en particular. Pero cómo se modela la relación entre <span class="math inline">\(p(X) = Pr(Y = 1|X)\)</span> y <em>Y</em>. Para encontrar estas probabilidades podemos usar un modelo de regresión lineal.</p>
<p><span class="math display">\[p(X) = β_{0} + β_{1}X\]</span></p>
<p>No obstante, al utilizar esta ecuación, podemos obtener resultados mayores que <em>1</em> y menores que <em>0</em>, por la tanto, las probabilidades deben de caer entre <em>0</em> y <em>1</em>. Para evitar que <span class="math inline">\(P(X) &lt; 0\)</span> para algunos valores de <em>X</em> y <span class="math inline">\(P(X) &lt; 1\)</span> para otros valores <em>X</em>, debemos modelar nuestra ecuación para que nuestros resultados caigan entre 0 y 1 para todos los valores de <em>X</em>. En regresiones logísticas, utilizamos la función logística, la cual está dada por</p>
<p><span class="math display">\[P(X) = \frac{e^{ β_{0} + β_{1}X }}{1 + e^{ β_{0} + β_{1}X }}\]</span></p>
<p>Para poder ajustar el modelo, utilizamos algo llamado método de <strong>máxima verosimilitud</strong>. Cuando graficamos la función logística en un plano, vamos a encontrar que va a dar una curva en <em>forma de S</em>.</p>
<p>La ecuación puede ser simplificada de la siguiente manera</p>
<p><span class="math display">\[\frac{P(X)}{1 – P(X)} = e^{ β_{0} + β_{1}X }\]</span></p>
<p>La cantidad dada por <span class="math inline">\(p(X)/[1 – p(X)]\)</span> es conocida como posibilidades, y puede tomar valores entre <span class="math inline">\(0\)</span> e <span class="math inline">\(\infty\)</span>. Valores de posibilidades hacer de <span class="math inline">\(0\)</span> e <span class="math inline">\(\infty\)</span> indica muy bajas o muy altas probabilidades, respectivamente. Si aplicamos logaritmo a ambos lados de la ecuación tendremos que</p>
<p><span class="math display">\[log(\frac{P(X)}{1 – p(X)} = β_{0} + β_{1}X)\]</span></p>
<p>La parte de la izquierda esta ecuación es conocida como <em>función logit</em>, <em>log-odds</em> o <em>logit</em>, que es básicamente la transformación de <span class="math inline">\(p(X)\)</span>. En los modelos de regresión logística, el incremento de una unidad de <em>X</em> cambia los log-odds por <span class="math inline">\(β_{1}\)</span>, esto también se puede ver como la multiplicación de las posibilidades por <span class="math inline">\(e^{β_{1}}\)</span>. <em>Note: dado que la relación entre <span class="math inline">\(p(X)\)</span> y <span class="math inline">\(X\)</span> no es una línea recta, <span class="math inline">\(β_{1}\)</span> <strong>no</strong> corresponde al cambio de <span class="math inline">\(p(X)\)</span> asociado con el incremento en una unidad en <span class="math inline">\(X\)</span></em>.</p>
<div id="estimación-de-coeficiente-en-regresiones-logísticas" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Estimación de coeficiente en regresiones logísticas<a href="clasificaciones.html#estimación-de-coeficiente-en-regresiones-logísticas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para el caso de las regesiones logísticas, los coeficientes <span class="math inline">\(β_{0}\)</span> y <span class="math inline">\(β_{1}\)</span> son desconocidos, así que los tenemos que estimar utilizando el set de datos de entrenamiento. Para esto utilizamos un método conocido como <strong>máxima verosimilitud</strong>. Este método busca encontrar un <span class="math inline">\(\hat{β_{0}}\)</span> y un <span class="math inline">\(\hat{β_{1}}\)</span> que al ser momento de ser incluidos en el modelo para <span class="math inline">\(p(X)\)</span> de como resultado números entre <em>0</em> y <em>1</em>. La ecuación de la <strong>función de la verosimilitud</strong> está dada por</p>
<p><span class="math display">\[l(β_{0}, β_{1}) = \prod_{i:y_{1} = 1}p(X) \prod_{i’:y_{i’=0}}(1-p(x_{i’}))\]</span></p>
<p>En donde los <span class="math inline">\(β_{0}\)</span> y <span class="math inline">\(β_{1}\)</span> que se van a elegir son aquellos que <em>maximicen</em> la función de la verosimilitud de los datos observados. La ecuación anterior, nos da la probabilidad de observador <em>0</em> y <em>1</em> en nuestros datos.</p>
<p>Al igual como ya lo hemos visto en los otros modelos, para el caso de la regresión logística, podemos medir que tan precisa fue la estimación de los coeficientes por medio del error estándar. Para ello, utilizamos el <strong>estadístico z</strong>. Para el caso del <em>estadístico z</em>, tenemos que está asociado con <span class="math inline">\(β_{1}\)</span> lo cual es igual a <span class="math inline">\(\hat{β_{1}}/SE(\hat{β_{1}})\)</span>. Con esto, podemos decir que un valor grande de <em>z</em> es evidencia en contra de la hipótesis nula <span class="math inline">\(H_{0}: β_{1}=0\)</span>, esto implicaría que la hipótesis nula es <span class="math inline">\(p(X)=\frac{e^{β_{0}}}{1+ e^{β_{0}}}\)</span>.</p>
</div>
<div id="regresiones-logísticas-múltiples-y-para-2-clases-de-respuestas" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Regresiones logísticas múltiples y para &gt;2 clases de respuestas<a href="clasificaciones.html#regresiones-logísticas-múltiples-y-para-2-clases-de-respuestas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para el caso de las regresiones logísticas, estas se pueden utilizar para predecir problemas de variables binarias usan múltiples predictores. Para ello, usamos la siguiente ecuación</p>
<p><span class="math display">\[log(\frac{p(X)}{1-p(X)}) = β_{0} + β_{1}X_{1} +…+ β_{p}X_{p}\]</span></p>
<p>Donde <span class="math inline">\(X = (X_{1},…,X_{p})\)</span> indican una cantidad <em>p</em> de predictores. Esta ecuación se puede reescribir como</p>
<p><span class="math display">\[p(X) = \frac{e^{ β_{0} + β_{1}X_{1}+…++ β_{p}X_{p}}}{1+ e^{β_{0} + β_{1}X_{1}+…++ β_{p}X_{p}}}\]</span></p>
<p>En este caso, utilizamos el método de <em>máxima verosimilitud</em> para estimar <span class="math inline">\(β_{0}\)</span>, <span class="math inline">\(β_{1}\)</span>,…, <span class="math inline">\(β_{p}\)</span>.</p>
<p><strong>Para poder explicar más a detalle esta sección, nos toca utilizar ejemplos, en especial para explicar lo de variables de confusión (confounding variables)</strong>.</p>
<p>Por otro lado, a veces necesitamos clasificar una variable respuesta que tiene mas de dos clases. Para esto usualmente usamos el <strong>análisis discriminante lineal</strong>.</p>
</div>
</div>
<div id="análisis-discriminante-lineal" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Análisis discriminante lineal<a href="clasificaciones.html#análisis-discriminante-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El <strong>análisis discriminante lineal</strong> (LDA por sus siglas en inglés, Linear Discriminant Analysis) es un método que busca determinar a qué grupo pertenece un individuo. Es decir, nosotros modelamos la distribución de los predictores <em>X</em> separadamente en cada una de las clases respuesta (<em>Y</em>), y luego usamos el <strong>teorema de Bayes</strong> para encontrar lo estimados para <span class="math inline">\(Pr(Y = k|X=x)\)</span>. Cuando se asume una distribución normal, este modelo se comparta de una manera similar a la regresión logística.</p>
<div id="teorema-de-bayes-para-las-clasificaciones" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Teorema de Bayes para las clasificaciones<a href="clasificaciones.html#teorema-de-bayes-para-las-clasificaciones" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cuando queremos clasificar una observación en una de las <em>K</em> clases, donde <span class="math inline">\(K \ge 2\)</span>. Es decir, la respuesta categórica de la variable <em>Y</em> puede tomar <em>K</em> valores distintos.</p>
<p>La <em>función de densidad</em> de <em>X</em> nos ayuda a encontrar cuando una observación es de la clase <em>k</em>th, esta se denota como <span class="math inline">\(f_{k}(X) ≡ Pr(X = x|Y = k)\)</span>. Esto quiere decir que si <span class="math inline">\(f_{k}(x)\)</span> es relativamente grande, la probabilidad de que uno observación pertenezca a clase <em>k</em>th es <span class="math inline">\(X ≈ x\)</span>, es decir, es alta. Por otro lado, si <span class="math inline">\(f_{k}(x)\)</span> es bajo, la probabilidad de que <em>k</em>th sea <span class="math inline">\(X ≈ x\)</span> es baja también. Por esto, el <em>Teorema de Bayes</em> dice que</p>
<p><span class="math display">\[Pr(Y = k|X = x) = \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}f_{l}(x)}\]</span></p>
<p>En donde <span class="math inline">\({\pi_{k}}\)</span> representa la probabilidad <em>a priori</em> de observar una observación elegida al azar que provenga de <em>k</em>th. <span class="math inline">\({\pi_{k}}\)</span> puede estimarse fácilmente si tenemos una muestra aleatoria de <em>Y</em>s de la población.</p>
<p><em>Nota: Nos referimos a <span class="math inline">\(p_{k}(x)\)</span> como la probabilidad <strong>posterior</strong> de una observación de <span class="math inline">\(X = x\)</span> de pertenecer a la clase <span class="math inline">\(k\)</span>th. Esto quiere decir la probabilidad de que la observación pertenezca a la clase <span class="math inline">\(k\)</span>th <span class="math inline">\(dada\)</span> el valor del predictor para esa observación</em></p>
</div>
<div id="análisis-discriminante-lineal-para-p-1" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Análisis discriminante lineal para <span class="math inline">\(p = 1\)</span><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Asumamos que solo tenemos un predictor, es decir, <span class="math inline">\(p = 1\)</span>. Para obtener un estimado de <span class="math inline">\(f_{k}(x)\)</span> usamos la ecuación anterior para poder estimar <span class="math inline">\(p_{k}(x)\)</span>. Lo que buscamos es clasificar una observación a la clase en la cual <span class="math inline">\(p_{k}(x)\)</span> es mas alto. Asumamos de nuevo <span class="math inline">\(f_{k}(x)\)</span> es <em>normal</em> o <em>Gausiana</em>. Con un solo estimador, la densidad normal toma la forma de</p>
<p><span class="math display">\[f_{k}(x) = \frac{1}{\sqrt{2\pi}\delta_{k}}exp(-\frac{1}{2\delta_{k}^{2}}(x-\mu_{k})^2)\]</span>
Donde <span class="math inline">\(\mu_{k}\)</span> y <span class="math inline">\(\delta_{k}^{2}\)</span> son la media y la varianza de los parámetros para las <em>k</em>th clases. Asumamos que la varianza esta compartida entre todas las <em>K</em> clases, es decir, <span class="math inline">\(\delta_{1}^{2} = …=\delta_{1}^{K}\)</span>, lo cual se puede denotar como <span class="math inline">\(\delta^{2}\)</span>, ahora reemplacemos algunos términos de las dos ecuaciones anteriores</p>
<p><span class="math display">\[p_{k}(x) = \frac{\pi_{k}\frac{1}{\sqrt{2\pi}\delta}exp(-\frac{1}{2\delta^{2}}(x-\mu_{k})^2)}{\sum_{l=1}^{K}\pi_{l}\frac{1}{\sqrt{2\pi}\delta}exp(-\frac{1}{2\delta^{2}}(x-\mu_{l})^2)}\]</span>
Esta ecuación puede ser simplificada y cancelaciones, pero primero debemos tener en cuenta que para clasificar el valor de <span class="math inline">\(X = x\)</span>, neces itamos encontrar la clases donde la función <span class="math inline">\(p_{k}(X)\)</span> sea mayor. Ahora, apliquemos logaritmos, simplifiquemos y cancelemos algunos términos que no depende de <em>k</em> de la ecuación anterior. Con esto podemos ver que es equivalente a asignar <em>x</em> a la clase con el puntaje discrimante más alto</p>
<p><span class="math display">\[\delta_{k}(x) = x ⋅ \frac{\mu_{k}}{\delta^{2}} - \frac{\mu_{k}^{2}}{2\delta^{2}} + log(\pi_{k})\]</span>
Aquí podemos notar que <span class="math inline">\(\delta_{k}(x)\)</span> es una función linear de <em>x</em>. Además, si hay <span class="math inline">\(K = 2\)</span> clases y <span class="math inline">\(\pi_{1} = \pi_{2} = 0.5\)</span>, podemos decir que el límite de decisión es</p>
<p><span class="math display">\[x = \frac{\mu_{1} + \mu_{2}}{2}\]</span>
En la práctica, no sabemos cual es el valor de algunos parámetros, (<span class="math inline">\(\mu_{K}\)</span>, <span class="math inline">\(\pi_{k}\)</span>, <span class="math inline">\(\delta^{2}\)</span>), pero tenemos los datos de entrenamiento, los cuales podemos usar para estimar esos parametros. Las ecuaciones para estimar esos parámetros estas dadas por las siguiente escuaciones, comenzando con la media</p>
<p><span class="math display">\[\hat\mu_{k} = \frac{1}{n_{k}} \sum_{i:y_{i} = k}x_{i}\]</span>
<span class="math inline">\(\hat\pi_{k}\)</span>, la cual es la probabilidad <em>a priori</em> de que una observación pertenezca a la <em>k</em>th clase, está dada por</p>
<p><span class="math display">\[\hat\pi_{k} = \frac{n_{k}}{n}\]</span>
La varianza está dada por</p>
<p><span class="math display">\[\hat\delta_{k} = \sum_{k=1}^{K}\frac{n_{k} - 1}{n - K} ⋅ \hat\delta_{k}^{2}\]</span>
Para estas tres ecuaciones, <em>n</em> denota el número total de observaciones de entrenamiento y <span class="math inline">\(n_{k}\)</span> el número de observaciones de entrenamiento en la clase <em>k</em>th.</p>
</div>
<div id="análisis-discriminante-lineal-para-p-1-1" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Análisis discriminante lineal para <span class="math inline">\(p &gt; 1\)</span><a href="clasificaciones.html#análisis-discriminante-lineal-para-p-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ahora, asumamos que nuestro clasificador LDA tiene multiples predictores. Para esto tendríamos que asumir que <span class="math inline">\(X = (X_{1}, X_{2},...,X_{p})\)</span> tiene una distribución <em>Gausiana multiple</em>, con una media vectorial para una clase específica y una matriz de covarianza común. <em>Nota: la distribució Gausiana multivariada asume que cada predictor individual tiene una distribución normal unidimensional con algún grado de correlación entre pares de predictores</em>. La <em>densidad Gausiana mulvariada</em> se define como</p>
<p><span class="math display">\[f(x) = \frac{1}{(2\pi)^{p/2}|\sum|^{1/2}}exp(-\frac{1}{2}(x - \mu)^{T}\sum^{-1}(x - \mu))\]</span>
Para esto caso, se asume que el clasificador LDA escoje una observación de la clase <em>k</em>th de una distribució Gausiana multivariada <span class="math inline">\(N(\mu_{k}, \sum)\)</span>, donde <span class="math inline">\(\mu_{k}\)</span> es el vector de una media clase específica, y <span class="math inline">\(\sum\)</span> es una matriz de covarianza que es común para todas las clases <em>K</em>. La función discriminante cuando <span class="math inline">\(p &gt; 1\)</span> se denota como</p>
<p><span class="math display">\[\delta_{k}(X) = x^{T}\sum^{-1}\mu_{k}-\frac{1}{2}\mu_{k}^{T}\sum^{-1}\mu_{k} + log\pi_{k}\]</span>
Al igual que en otros clasificadores, tenemos que estimar los parámetrros que son desconocidos <span class="math inline">\(\mu_{1},...,\mu_{k}\)</span>, <span class="math inline">\(\pi_{1},...,\pi_{k}\)</span>, <span class="math inline">\(\sum\)</span>), las formulas para hallar estos parámetros son parecidas a las que vimos anteriormente para LDA de un solo predictor. Asimismo, para el caso de LDA buscamos que el modelo tengo una tasa de error baja al momento de clasificar, sin embargo, hay un par de salvedades</p>
<ul>
<li>Los erros de entrenamiento usualmente van a ser más bajo que los erros de prueba. En otras palabras, entre más alta seta se la relación de los parámetros <em>p</em> con el número de muestras <em>n</em>, más posibilidad hay de que el sobreajuste tenga un rol en el error.</li>
<li>El clasifcador <em>nulo</em> va a tener una tasa de error que solo es un poquito más alta que la tasa de error del set de entrenamiento de LDA.</li>
</ul>
<p>Cuando estamos corriendo nuestro LDA nos podemos encontrar con dos tipos de error:</p>
<ul>
<li><em>Falso positivo</em>: que son la fracción de ejemplos negativos que fueron clasificados como positivos.</li>
<li><em>Falso negativo</em>: que son la fracción de ejemplos positivos que fueron clasificados como negativos.</li>
</ul>
<p>Durante nuestro análisis de LDA lo que queremos es reducir la tasa del error lo más posible, para esto buscamos encontrar el punto en el cual el error general, el de falsos negativos y el de falsos positivos sea el más bajo.</p>
<p>Una de las gráficas más populares para visualizar dos tipos de errores al mismo tiempo en la <em>curva ROC</em> (por sus siglas en inglés, receiver operating characterictics). Con esta podemos ver qué efectivo ha isdo nuestor clasificador, el cual resume todos los posibles umbrales, en una parte de la curva conocida como <em>area bajo la curva</em> o <em>AUC</em> (por sus siglas en inglésl area under the curve); entre más alto el AUC mejor.</p>
</div>
<div id="análisis-discrimimante-cuadrático" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Análisis discrimimante cuadrático<a href="clasificaciones.html#análisis-discrimimante-cuadrático" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Usualmente usamos el análisis discrimimante cuadrático o QDA (por sus siglas en inglés; quadratic disminant analysis) cuando las observaciones dentro de clase son elegidas de una distribución Gausiana multivariada con una media clase-específica y una matriz de covarianza común para todas las clases <em>K</em>. QDA asume que cada calse tiene su propia matriz de covarianza. Es decir, cada observación de de la clase <em>k</em>th está por la forma <span class="math inline">\(X \sim {\sf N}(\mu_{k}, \sum_{k})\)</span>, donde <span class="math inline">\(\sum_{k}\)</span> es una matriz de covarianza para la clase <em>k</em>th. Bajo estas asunciones, el clasificador de Bayes asigna para una observación <span class="math inline">\(X = x\)</span> a una clase donde <span class="math inline">\(\delta_{k}(x)\)</span> sea más alto, esto está dado por</p>
<p><span class="math display">\[\delta_{k}(x) = -\frac{1}{2}(x-\mu_{k}^{T})\sum_{k}^{-1}(x-\mu_{k}^{T})+log\pi_{k}-\frac{1}{2}log|\sum_{k}|\]</span>
QDA estima una matriz de varianza separada para cada clase, para un total de <span class="math inline">\(Kp(p+1)/2\)</span> parámetros. Esto hace que QDA sea un clasificador más flexible que LDA. Usualmente se recomienda utilizar QDA si el set de entrenaminto es grande, para que a así la varianza del clasificador no sea un problema o por si la asunción de tener una matriz de covarianza común para las cada clase <em>K</em> no se puede obtener.</p>
</div>
</div>
<div id="clasificador-bayesiano-ingenuo" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Clasificador bayesiano ingenuo<a href="clasificaciones.html#clasificador-bayesiano-ingenuo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El clasificador bayesiano ingueno o naive Bayes asume que todas las características son independientes en cda clase. Este es particularmente útil cuando <em>p</em> es muy grande y otros métodos como LDA y QDA no son capaces de manejar tantos datos.</p>
<p>El naive Bayes gausiana sume que <span class="math inline">\(\sum_{k}\)</span> es diagonal, es decir</p>
<p><span class="math display">\[\delta_{k}(x) = \frac{1}{2}\sum_{j=1}^{p}[\frac{(x_{j} - \mu_{kj})^{2}}{\delta_{kj}^{2}}] + log\pi_{k}\]</span>
Asimismo, el naive Bayes se puede usar para vectores con caracteríticas mixtas (cualitativas y cuantitativas). Si <span class="math inline">\(X_{j}\)</span> es cualitativa se puede reemplezar <span class="math inline">\(f_{kj}(x_{j})\)</span> con la función de masa de probabilida (histograma) sobre categorías discretas. <em>Nota: a pesar de las asunciones, el naive Bayes usualmente produce bueno resultados de clasificación</em>.</p>
</div>
<div id="k-vecinos-más-cercanos" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> K-vecinos más cercanos<a href="clasificaciones.html#k-vecinos-más-cercanos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En capítulos anteriores hablamos brevemente sobre K-vecinos más cercanos o KNN (por sus siglas en inglés, k-nearest neighbors). Para hacer predicciones para un observación de <span class="math inline">\(X = x\)</span>, las observaciones <em>K</em> de entrenamiento que son cercanas a <em>x</em> son identificadas. Luego, <em>X</em> es asignada a una de las clases cuya plurarlidad corresponda a una de las observaciones a las que pertenece. Es decir que KNN es una método completamente no paramétrico. Este método requiere la selección de <em>K</em>, los vecinos más cercanos, es decir, realizamos el KNN con dos valores de <span class="math inline">\(K: K = 1\)</span> y un valor de <em>K</em> que es escogido automaticamente usando un método llamado <em>validación cruzada</em>, del cual hablaremos de manera más intensiva en el próximo capítulo.</p>
<p><em><strong>Nota</strong>: KNN no hace asunsiones somre la forma de la límite de decisión, además de no decirnos cual(es) de los predictores son los más importantes. Igualmente, este método es mucho más flexible que QDA</em>.</p>
</div>
<div id="comparación-de-métodos-de-clasificación" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Comparación de métodos de clasificación<a href="clasificaciones.html#comparación-de-métodos-de-clasificación" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Anteriormente vimos como se pueden utilizar cada uno de los métodos de clasificación, al igual de los parámetros y las asunciones que tienen en cuenta. Sin embargo, cada uno de estos se puede utilizar dependiendo de lo que estemos buscando. Aquí vamos a dar en resumen de que implicada cada uno de estos métodos.</p>
<ul>
<li>La regresión logística es ampliamente usada como la clasificación, pero en particular cuando se tiene que <span class="math inline">\(k = 2\)</span>.</li>
<li>LDA es útil cuando <em>n</em> es pequeño o cuando las calses están bien separadas, y las asuncions gausianas son razonables. Además, cuando <span class="math inline">\(k &gt; 2\)</span>.</li>
<li>Naive Bayes es particularmente útil cuando <em>p</em> es muy grande.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="capítulo-3-regresiones-lineales.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="métodos-de-remuestreo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/RomanPalacios-lab/ml-biologia/edit/main/04-cla.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/RomanPalacios-lab/ml-biologia/blob/main/04-cla.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
